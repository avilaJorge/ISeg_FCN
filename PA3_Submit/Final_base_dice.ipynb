{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-U6GPrYjHErx"
   },
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1581814520205,
     "user": {
      "displayName": "Chaoqi Xu",
      "photoUrl": "",
      "userId": "02369116997767525254"
     },
     "user_tz": 480
    },
    "id": "1-DLo_fHAs4W",
    "outputId": "1f68a827-d5f6-46bc-e692-b4a724dd1a66",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "total GPU Mem:  17071734784\n",
      "total GPU Cached:  0\n",
      "total GPU Allocated:  0\n",
      "Available GB:  17.071734784\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "def print_GPU_stats():\n",
    "    print(\"total GPU Mem: \", torch.cuda.get_device_properties(device).total_memory)\n",
    "    print(\"total GPU Cached: \", torch.cuda.memory_cached(device))\n",
    "    print(\"total GPU Allocated: \", torch.cuda.memory_allocated(device))\n",
    "    print(\"Available GB: \", (torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device))/(10**9))\n",
    "print_GPU_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPkHApt7SnCk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW1tM2upAs4U"
   },
   "outputs": [],
   "source": [
    "# Change name to FCN to use this model instead I think\n",
    "\n",
    "class FCN_bak(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(FCN_bak, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.conv1   = nn.Conv2d(3, 32, kernel_size=(3,5), stride=(2,4), padding=1, dilation=1)\n",
    "        self.bnd1    = nn.BatchNorm2d(32)\n",
    "        self.conv2   = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd2    = nn.BatchNorm2d(64)\n",
    "        self.conv3   = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd3    = nn.BatchNorm2d(128)\n",
    "        self.conv4   = nn.Conv2d(128,256, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd4    = nn.BatchNorm2d(256)\n",
    "        self.conv5   = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd5    = nn.BatchNorm2d(512)\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(256)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(128)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(64)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(32)\n",
    "        self.deconv5  = nn.ConvTranspose2d(32, 3, kernel_size=(3, 5), stride=(2,4), padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5= nn.BatchNorm2d(3)\n",
    "        self.classifier = nn.Conv2d(3,n_class, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pool = nn.MaxPool2d(2, stride=2,return_indices = True)\n",
    "        unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "        \n",
    "        x1, indice1 = pool(self.relu(self.conv1(x)))\n",
    "        x2, indice2 = pool(self.relu(self.conv2(self.bnd1(x1))))\n",
    "        x3, indice3 = pool(self.relu(self.conv3(self.bnd2(x2))))\n",
    "        x4, indice4 = pool(self.relu(self.conv4(self.bnd3(x3))))\n",
    "        x5, indice5 = pool(self.relu(self.conv5(self.bnd4(x4))))\n",
    "        \n",
    "        z1 = self.deconv1(self.bnd5(self.relu(unpool((x5), indice5))))\n",
    "        z2 = self.deconv2(self.bn1(self.relu(unpool((z1), indice4))))\n",
    "        z3 = self.deconv3(self.bn2(self.relu(unpool((z2), indice3))))\n",
    "        z4 = self.deconv4(self.bn3(self.relu(unpool((z3), indice2))))\n",
    "        z5 = self.deconv5(self.bn4(self.relu(unpool((z4), indice1))))\n",
    "        \n",
    "        out_decoder = self.classifier(self.bn5(z5))                  \n",
    "\n",
    "        return out_decoder  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujBPdFdaqfzi"
   },
   "outputs": [],
   "source": [
    "class AverageBase(object):\n",
    "    \n",
    "    def __init__(self, value=0):\n",
    "        self.value = float(value) if value is not None else None\n",
    "       \n",
    "    def __str__(self):\n",
    "        return str(round(self.value, 4))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __format__(self, fmt):\n",
    "        return self.value.__format__(fmt)\n",
    "    \n",
    "    def __float__(self):\n",
    "        return self.value\n",
    "    \n",
    "\n",
    "class RunningAverage(AverageBase):\n",
    "    \"\"\"\n",
    "    Keeps track of a cumulative moving average (CMA).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, value=0, count=0):\n",
    "        super(RunningAverage, self).__init__(value)\n",
    "        self.count = count\n",
    "        \n",
    "    def update(self, value):\n",
    "        self.value = (self.value * self.count + float(value))\n",
    "        self.count += 1\n",
    "        self.value /= self.count\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class MovingAverage(AverageBase):\n",
    "    \"\"\"\n",
    "    An exponentially decaying moving average (EMA).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.99):\n",
    "        super(MovingAverage, self).__init__(None)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def update(self, value):\n",
    "        if self.value is None:\n",
    "            self.value = float(value)\n",
    "        else:\n",
    "            self.value = self.alpha * self.value + (1 - self.alpha) * float(value)\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gndmee31As4Y"
   },
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "num_wrkrs= 2\n",
    "train_dataset = CityScapesDataset(csv_file='train_local.csv')\n",
    "val_dataset = CityScapesDataset(csv_file='val_local.csv')\n",
    "test_dataset = CityScapesDataset(csv_file='test_local.csv')\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_wrkrs,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_wrkrs,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_wrkrs,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxo9AEgcAs4a"
   },
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    smooth = 1e-7\n",
    "    num = pred.size(0)\n",
    "    # print(pred.size())\n",
    "    # print(target.size())\n",
    "    m1 = pred.reshape(num, -1)  # Flatten\n",
    "    m2 = target.reshape(num, -1)  # Flatten\n",
    "    # print(m1.size())\n",
    "    intersection = (m1 * m2).sum()\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdU43gWpAs4d"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "epochs     = 100\n",
    "start_epoch = 30\n",
    "fcn_model = FCN_bak(n_class=34)\n",
    "# fcn_model.apply(init_weights)\n",
    "fcn_model.load_state_dict(torch.load('/content/drive/My Drive/CSE253/PA3/Dice/model_02_15_23_42.pt'))\n",
    "optimizer = optim.Adam(fcn_model.parameters(), lr = 1e-4, weight_decay= 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gipH8gimAs4e"
   },
   "outputs": [],
   "source": [
    "dt = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "output_fn = \"/content/drive/My Drive/CSE253/PA3/Dice/model_output_\" + dt + \".txt\"\n",
    "best_model_fn = \"/content/drive/My Drive/CSE253/PA3/Dice/best_model_\" + dt + \".pt\"\n",
    "model_fn = \"/content/drive/My Drive/CSE253/PA3/Dice/model_\" + dt + \".pt\"\n",
    "\n",
    "def print_info(out_str):\n",
    "    f = open(output_fn,\"a\")\n",
    "    print(out_str)\n",
    "    f.write(out_str)\n",
    "    f.close()\n",
    "\n",
    "# print_info(\"Started: %s\\nFrom a previously trained model which left off on start of epoch 9.\\n\" % datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753299,
     "status": "error",
     "timestamp": 1581581270207,
     "user": {
      "displayName": "Jorge Avila",
      "photoUrl": "",
      "userId": "09136444355201812303"
     },
     "user_tz": 480
    },
    "id": "hk1Q_oWZMkNP",
    "outputId": "c819e482-8c87-4ef1-b2c5-3bb0d858fdce",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU works\n",
      "Starting Training\n",
      "epoch30, iter0, loss: 0.471845805644989 \n",
      "\n",
      "epoch30, iter10, loss: 0.48630237579345703 \n",
      "\n",
      "epoch30, iter20, loss: 0.5327383279800415 \n",
      "\n",
      "epoch30, iter30, loss: 0.4254082441329956 \n",
      "\n",
      "epoch30, iter40, loss: 0.5251901149749756 \n",
      "\n",
      "epoch30, iter50, loss: 0.5396333932876587 \n",
      "\n",
      "epoch30, iter60, loss: 0.49754029512405396 \n",
      "\n",
      "epoch30, iter70, loss: 0.5022299289703369 \n",
      "\n",
      "epoch30, iter80, loss: 0.471383273601532 \n",
      "\n",
      "epoch30, iter90, loss: 0.49218785762786865 \n",
      "\n",
      "epoch30, iter100, loss: 0.4511209726333618 \n",
      "\n",
      "epoch30, iter110, loss: 0.48105841875076294 \n",
      "\n",
      "epoch30, iter120, loss: 0.5135829448699951 \n",
      "\n",
      "epoch30, iter130, loss: 0.43207961320877075 \n",
      "\n",
      "epoch30, iter140, loss: 0.45331746339797974 \n",
      "\n",
      "epoch30, iter150, loss: 0.4256807565689087 \n",
      "\n",
      "epoch30, iter160, loss: 0.45807355642318726 \n",
      "\n",
      "epoch30, iter170, loss: 0.4973229765892029 \n",
      "\n",
      "epoch30, iter180, loss: 0.5460232496261597 \n",
      "\n",
      "epoch30, iter190, loss: 0.44363176822662354 \n",
      "\n",
      "epoch30, iter200, loss: 0.5729594230651855 \n",
      "\n",
      "epoch30, iter210, loss: 0.5303155183792114 \n",
      "\n",
      "epoch30, iter220, loss: 0.4946395754814148 \n",
      "\n",
      "epoch30, iter230, loss: 0.5114521980285645 \n",
      "\n",
      "epoch30, iter240, loss: 0.4459872841835022 \n",
      "\n",
      "epoch30, iter250, loss: 0.5343153476715088 \n",
      "\n",
      "epoch30, iter260, loss: 0.5692582130432129 \n",
      "\n",
      "epoch30, iter270, loss: 0.42280566692352295 \n",
      "\n",
      "epoch30, iter280, loss: 0.4817362427711487 \n",
      "\n",
      "epoch30, iter290, loss: 0.4501100778579712 \n",
      "\n",
      "epoch30, iter300, loss: 0.5203911066055298 \n",
      "\n",
      "epoch30, iter310, loss: 0.42979663610458374 \n",
      "\n",
      "epoch30, iter320, loss: 0.505119800567627 \n",
      "\n",
      "epoch30, iter330, loss: 0.5487631559371948 \n",
      "\n",
      "epoch30, iter340, loss: 0.532996654510498 \n",
      "\n",
      "epoch30, iter350, loss: 0.3914111256599426 \n",
      "\n",
      "epoch30, iter360, loss: 0.4858630895614624 \n",
      "\n",
      "epoch30, iter370, loss: 0.5292724370956421 \n",
      "\n",
      "epoch30, iter380, loss: 0.5142130255699158 \n",
      "\n",
      "epoch30, iter390, loss: 0.5615143775939941 \n",
      "\n",
      "epoch30, iter400, loss: 0.45383530855178833 \n",
      "\n",
      "epoch30, iter410, loss: 0.478720486164093 \n",
      "\n",
      "epoch30, iter420, loss: 0.507982075214386 \n",
      "\n",
      "Finish epoch 30, time elapsed 1425.8188350200653 \n",
      "\n",
      "Training Check:\tLoss: 0.490526\tAccuracy: 51.008214\tIoU: 0.179533 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 288.064318\n",
      "Best Loss: 0.5001666007770432\n",
      "\n",
      "Validation Results: Loss: 0.500167\tAccuracy: 50.057721\tIoU: 0.175665 \n",
      "\n",
      "epoch31, iter0, loss: 0.47520971298217773 \n",
      "\n",
      "epoch31, iter10, loss: 0.4682326912879944 \n",
      "\n",
      "epoch31, iter20, loss: 0.43885278701782227 \n",
      "\n",
      "epoch31, iter30, loss: 0.4287737011909485 \n",
      "\n",
      "epoch31, iter40, loss: 0.4583243727684021 \n",
      "\n",
      "epoch31, iter50, loss: 0.4621111750602722 \n",
      "\n",
      "epoch31, iter60, loss: 0.5065709948539734 \n",
      "\n",
      "epoch31, iter70, loss: 0.4952254891395569 \n",
      "\n",
      "epoch31, iter80, loss: 0.5092828273773193 \n",
      "\n",
      "epoch31, iter90, loss: 0.4706360101699829 \n",
      "\n",
      "epoch31, iter100, loss: 0.5185509324073792 \n",
      "\n",
      "epoch31, iter110, loss: 0.5358341932296753 \n",
      "\n",
      "epoch31, iter120, loss: 0.5611732006072998 \n",
      "\n",
      "epoch31, iter130, loss: 0.4445001482963562 \n",
      "\n",
      "epoch31, iter140, loss: 0.5374454259872437 \n",
      "\n",
      "epoch31, iter150, loss: 0.4787871241569519 \n",
      "\n",
      "epoch31, iter160, loss: 0.4940173029899597 \n",
      "\n",
      "epoch31, iter170, loss: 0.5271763801574707 \n",
      "\n",
      "epoch31, iter180, loss: 0.47355639934539795 \n",
      "\n",
      "epoch31, iter190, loss: 0.4090242385864258 \n",
      "\n",
      "epoch31, iter200, loss: 0.5209090113639832 \n",
      "\n",
      "epoch31, iter210, loss: 0.4992606043815613 \n",
      "\n",
      "epoch31, iter220, loss: 0.4351131319999695 \n",
      "\n",
      "epoch31, iter230, loss: 0.512373685836792 \n",
      "\n",
      "epoch31, iter240, loss: 0.4653211832046509 \n",
      "\n",
      "epoch31, iter250, loss: 0.4558259844779968 \n",
      "\n",
      "epoch31, iter260, loss: 0.48858100175857544 \n",
      "\n",
      "epoch31, iter270, loss: 0.6334215998649597 \n",
      "\n",
      "epoch31, iter280, loss: 0.5109738111495972 \n",
      "\n",
      "epoch31, iter290, loss: 0.510089635848999 \n",
      "\n",
      "epoch31, iter300, loss: 0.5382462739944458 \n",
      "\n",
      "epoch31, iter310, loss: 0.42940419912338257 \n",
      "\n",
      "epoch31, iter320, loss: 0.49037569761276245 \n",
      "\n",
      "epoch31, iter330, loss: 0.4768701195716858 \n",
      "\n",
      "epoch31, iter340, loss: 0.45616596937179565 \n",
      "\n",
      "epoch31, iter350, loss: 0.4591464400291443 \n",
      "\n",
      "epoch31, iter360, loss: 0.45183414220809937 \n",
      "\n",
      "epoch31, iter370, loss: 0.5061293244361877 \n",
      "\n",
      "epoch31, iter380, loss: 0.47220855951309204 \n",
      "\n",
      "epoch31, iter390, loss: 0.557532787322998 \n",
      "\n",
      "epoch31, iter400, loss: 0.5243142247200012 \n",
      "\n",
      "epoch31, iter410, loss: 0.549997091293335 \n",
      "\n",
      "epoch31, iter420, loss: 0.46797430515289307 \n",
      "\n",
      "Finish epoch 31, time elapsed 1452.919246673584 \n",
      "\n",
      "Training Check:\tLoss: 0.489119\tAccuracy: 51.165957\tIoU: 0.179797 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 287.585336\n",
      "Validation Results: Loss: 0.501280\tAccuracy: 49.955286\tIoU: 0.174945 \n",
      "\n",
      "epoch32, iter0, loss: 0.42165958881378174 \n",
      "\n",
      "epoch32, iter10, loss: 0.4453633427619934 \n",
      "\n",
      "epoch32, iter20, loss: 0.36565685272216797 \n",
      "\n",
      "epoch32, iter30, loss: 0.5105555653572083 \n",
      "\n",
      "epoch32, iter40, loss: 0.42835837602615356 \n",
      "\n",
      "epoch32, iter50, loss: 0.44170671701431274 \n",
      "\n",
      "epoch32, iter60, loss: 0.5368121862411499 \n",
      "\n",
      "epoch32, iter70, loss: 0.5077462196350098 \n",
      "\n",
      "epoch32, iter80, loss: 0.49714094400405884 \n",
      "\n",
      "epoch32, iter90, loss: 0.551571786403656 \n",
      "\n",
      "epoch32, iter100, loss: 0.5356361269950867 \n",
      "\n",
      "epoch32, iter110, loss: 0.4807280898094177 \n",
      "\n",
      "epoch32, iter120, loss: 0.5027602910995483 \n",
      "\n",
      "epoch32, iter130, loss: 0.5353147983551025 \n",
      "\n",
      "epoch32, iter140, loss: 0.4963998794555664 \n",
      "\n",
      "epoch32, iter150, loss: 0.504395604133606 \n",
      "\n",
      "epoch32, iter160, loss: 0.457133948802948 \n",
      "\n",
      "epoch32, iter170, loss: 0.5330806970596313 \n",
      "\n",
      "epoch32, iter180, loss: 0.5054149627685547 \n",
      "\n",
      "epoch32, iter190, loss: 0.5655770301818848 \n",
      "\n",
      "epoch32, iter200, loss: 0.39432036876678467 \n",
      "\n",
      "epoch32, iter210, loss: 0.4369911551475525 \n",
      "\n",
      "epoch32, iter220, loss: 0.40067052841186523 \n",
      "\n",
      "epoch32, iter230, loss: 0.41634541749954224 \n",
      "\n",
      "epoch32, iter240, loss: 0.48795801401138306 \n",
      "\n",
      "epoch32, iter250, loss: 0.4903193712234497 \n",
      "\n",
      "epoch32, iter260, loss: 0.4360455274581909 \n",
      "\n",
      "epoch32, iter270, loss: 0.5358966588973999 \n",
      "\n",
      "epoch32, iter280, loss: 0.4156545400619507 \n",
      "\n",
      "epoch32, iter290, loss: 0.487237811088562 \n",
      "\n",
      "epoch32, iter300, loss: 0.5098015666007996 \n",
      "\n",
      "epoch32, iter310, loss: 0.49573153257369995 \n",
      "\n",
      "epoch32, iter320, loss: 0.5886019468307495 \n",
      "\n",
      "epoch32, iter330, loss: 0.5067878365516663 \n",
      "\n",
      "epoch32, iter340, loss: 0.5918881297111511 \n",
      "\n",
      "epoch32, iter350, loss: 0.5179848670959473 \n",
      "\n",
      "epoch32, iter360, loss: 0.4673527479171753 \n",
      "\n",
      "epoch32, iter370, loss: 0.5229095220565796 \n",
      "\n",
      "epoch32, iter380, loss: 0.5209565162658691 \n",
      "\n",
      "epoch32, iter390, loss: 0.4531450867652893 \n",
      "\n",
      "epoch32, iter400, loss: 0.44830411672592163 \n",
      "\n",
      "epoch32, iter410, loss: 0.5106666088104248 \n",
      "\n",
      "epoch32, iter420, loss: 0.5685925483703613 \n",
      "\n",
      "Finish epoch 32, time elapsed 1478.3541793823242 \n",
      "\n",
      "Training Check:\tLoss: 0.492143\tAccuracy: 50.874872\tIoU: 0.178950 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 291.340919\n",
      "Validation Results: Loss: 0.501736\tAccuracy: 49.924588\tIoU: 0.174865 \n",
      "\n",
      "epoch33, iter0, loss: 0.4106476902961731 \n",
      "\n",
      "epoch33, iter10, loss: 0.4271312952041626 \n",
      "\n",
      "epoch33, iter20, loss: 0.5355527400970459 \n",
      "\n",
      "epoch33, iter30, loss: 0.44095176458358765 \n",
      "\n",
      "epoch33, iter40, loss: 0.5108275413513184 \n",
      "\n",
      "epoch33, iter50, loss: 0.44306057691574097 \n",
      "\n",
      "epoch33, iter60, loss: 0.46188998222351074 \n",
      "\n",
      "epoch33, iter70, loss: 0.48620855808258057 \n",
      "\n",
      "epoch33, iter80, loss: 0.5025277137756348 \n",
      "\n",
      "epoch33, iter90, loss: 0.4558008313179016 \n",
      "\n",
      "epoch33, iter100, loss: 0.45203697681427 \n",
      "\n",
      "epoch33, iter110, loss: 0.5078191757202148 \n",
      "\n",
      "epoch33, iter120, loss: 0.46862471103668213 \n",
      "\n",
      "epoch33, iter130, loss: 0.47131264209747314 \n",
      "\n",
      "epoch33, iter140, loss: 0.4776265621185303 \n",
      "\n",
      "epoch33, iter150, loss: 0.43659746646881104 \n",
      "\n",
      "epoch33, iter160, loss: 0.5534753799438477 \n",
      "\n",
      "epoch33, iter170, loss: 0.4908323884010315 \n",
      "\n",
      "epoch33, iter180, loss: 0.46527689695358276 \n",
      "\n",
      "epoch33, iter190, loss: 0.4933174252510071 \n",
      "\n",
      "epoch33, iter200, loss: 0.4230004549026489 \n",
      "\n",
      "epoch33, iter210, loss: 0.5319697856903076 \n",
      "\n",
      "epoch33, iter220, loss: 0.5665879249572754 \n",
      "\n",
      "epoch33, iter230, loss: 0.5242980122566223 \n",
      "\n",
      "epoch33, iter240, loss: 0.43520814180374146 \n",
      "\n",
      "epoch33, iter250, loss: 0.497946560382843 \n",
      "\n",
      "epoch33, iter260, loss: 0.5067803263664246 \n",
      "\n",
      "epoch33, iter270, loss: 0.512252926826477 \n",
      "\n",
      "epoch33, iter280, loss: 0.48923879861831665 \n",
      "\n",
      "epoch33, iter290, loss: 0.43779319524765015 \n",
      "\n",
      "epoch33, iter300, loss: 0.5196701884269714 \n",
      "\n",
      "epoch33, iter310, loss: 0.4461812376976013 \n",
      "\n",
      "epoch33, iter320, loss: 0.42902690172195435 \n",
      "\n",
      "epoch33, iter330, loss: 0.5129718780517578 \n",
      "\n",
      "epoch33, iter340, loss: 0.5352334380149841 \n",
      "\n",
      "epoch33, iter350, loss: 0.5309925079345703 \n",
      "\n",
      "epoch33, iter360, loss: 0.5197610855102539 \n",
      "\n",
      "epoch33, iter370, loss: 0.44530099630355835 \n",
      "\n",
      "epoch33, iter380, loss: 0.4954138994216919 \n",
      "\n",
      "epoch33, iter390, loss: 0.5511276721954346 \n",
      "\n",
      "epoch33, iter400, loss: 0.4212172031402588 \n",
      "\n",
      "epoch33, iter410, loss: 0.45892852544784546 \n",
      "\n",
      "epoch33, iter420, loss: 0.5195990800857544 \n",
      "\n",
      "Finish epoch 33, time elapsed 1496.6474242210388 \n",
      "\n",
      "Training Check:\tLoss: 0.496067\tAccuracy: 50.496638\tIoU: 0.177434 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 295.309517\n",
      "Validation Results: Loss: 0.503462\tAccuracy: 49.767648\tIoU: 0.173825 \n",
      "\n",
      "epoch34, iter0, loss: 0.5095556974411011 \n",
      "\n",
      "epoch34, iter10, loss: 0.47192156314849854 \n",
      "\n",
      "epoch34, iter20, loss: 0.44234418869018555 \n",
      "\n",
      "epoch34, iter30, loss: 0.5518238544464111 \n",
      "\n",
      "epoch34, iter40, loss: 0.5308268666267395 \n",
      "\n",
      "epoch34, iter50, loss: 0.4784137010574341 \n",
      "\n",
      "epoch34, iter60, loss: 0.5168125629425049 \n",
      "\n",
      "epoch34, iter70, loss: 0.5245219469070435 \n",
      "\n",
      "epoch34, iter80, loss: 0.4965190291404724 \n",
      "\n",
      "epoch34, iter90, loss: 0.5432696342468262 \n",
      "\n",
      "epoch34, iter100, loss: 0.5024269819259644 \n",
      "\n",
      "epoch34, iter110, loss: 0.42391252517700195 \n",
      "\n",
      "epoch34, iter120, loss: 0.5403298735618591 \n",
      "\n",
      "epoch34, iter130, loss: 0.47715461254119873 \n",
      "\n",
      "epoch34, iter140, loss: 0.5276780128479004 \n",
      "\n",
      "epoch34, iter150, loss: 0.4923878312110901 \n",
      "\n",
      "epoch34, iter160, loss: 0.5021913051605225 \n",
      "\n",
      "epoch34, iter170, loss: 0.4413423538208008 \n",
      "\n",
      "epoch34, iter180, loss: 0.5515670776367188 \n",
      "\n",
      "epoch34, iter190, loss: 0.5412724018096924 \n",
      "\n",
      "epoch34, iter200, loss: 0.5206024646759033 \n",
      "\n",
      "epoch34, iter210, loss: 0.5237456560134888 \n",
      "\n",
      "epoch34, iter220, loss: 0.49070823192596436 \n",
      "\n",
      "epoch34, iter230, loss: 0.48486465215682983 \n",
      "\n",
      "epoch34, iter240, loss: 0.5853712558746338 \n",
      "\n",
      "epoch34, iter250, loss: 0.4933850169181824 \n",
      "\n",
      "epoch34, iter260, loss: 0.48009276390075684 \n",
      "\n",
      "epoch34, iter270, loss: 0.47498244047164917 \n",
      "\n",
      "epoch34, iter280, loss: 0.4743701219558716 \n",
      "\n",
      "epoch34, iter290, loss: 0.5157649517059326 \n",
      "\n",
      "epoch34, iter300, loss: 0.52201247215271 \n",
      "\n",
      "epoch34, iter310, loss: 0.479079008102417 \n",
      "\n",
      "epoch34, iter320, loss: 0.46601027250289917 \n",
      "\n",
      "epoch34, iter330, loss: 0.5374263525009155 \n",
      "\n",
      "epoch34, iter340, loss: 0.5068297982215881 \n",
      "\n",
      "epoch34, iter350, loss: 0.5003964304924011 \n",
      "\n",
      "epoch34, iter360, loss: 0.43366116285324097 \n",
      "\n",
      "epoch34, iter370, loss: 0.3324301838874817 \n",
      "\n",
      "epoch34, iter380, loss: 0.4908459186553955 \n",
      "\n",
      "epoch34, iter390, loss: 0.49504023790359497 \n",
      "\n",
      "epoch34, iter400, loss: 0.5344483852386475 \n",
      "\n",
      "epoch34, iter410, loss: 0.4526747465133667 \n",
      "\n",
      "epoch34, iter420, loss: 0.4949440360069275 \n",
      "\n",
      "Finish epoch 34, time elapsed 1538.3110721111298 \n",
      "\n",
      "Training Check:\tLoss: 0.493520\tAccuracy: 50.764188\tIoU: 0.177876 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 300.371564\n",
      "Validation Results: Loss: 0.506207\tAccuracy: 49.498348\tIoU: 0.172965 \n",
      "\n",
      "epoch35, iter0, loss: 0.4824976325035095 \n",
      "\n",
      "epoch35, iter10, loss: 0.4896038770675659 \n",
      "\n",
      "epoch35, iter20, loss: 0.535327672958374 \n",
      "\n",
      "epoch35, iter30, loss: 0.47955000400543213 \n",
      "\n",
      "epoch35, iter40, loss: 0.4387044310569763 \n",
      "\n",
      "epoch35, iter50, loss: 0.4457288980484009 \n",
      "\n",
      "epoch35, iter60, loss: 0.5038892030715942 \n",
      "\n",
      "epoch35, iter70, loss: 0.45771992206573486 \n",
      "\n",
      "epoch35, iter80, loss: 0.5724008083343506 \n",
      "\n",
      "epoch35, iter90, loss: 0.5339560508728027 \n",
      "\n",
      "epoch35, iter100, loss: 0.45490866899490356 \n",
      "\n",
      "epoch35, iter110, loss: 0.39329880475997925 \n",
      "\n",
      "epoch35, iter120, loss: 0.47969257831573486 \n",
      "\n",
      "epoch35, iter130, loss: 0.542107105255127 \n",
      "\n",
      "epoch35, iter140, loss: 0.47072428464889526 \n",
      "\n",
      "epoch35, iter150, loss: 0.4486004710197449 \n",
      "\n",
      "epoch35, iter160, loss: 0.5564631223678589 \n",
      "\n",
      "epoch35, iter170, loss: 0.48033422231674194 \n",
      "\n",
      "epoch35, iter180, loss: 0.5204734802246094 \n",
      "\n",
      "epoch35, iter190, loss: 0.46481579542160034 \n",
      "\n",
      "epoch35, iter200, loss: 0.4346507787704468 \n",
      "\n",
      "epoch35, iter210, loss: 0.4926658272743225 \n",
      "\n",
      "epoch35, iter220, loss: 0.5808313488960266 \n",
      "\n",
      "epoch35, iter230, loss: 0.45843613147735596 \n",
      "\n",
      "epoch35, iter240, loss: 0.44329047203063965 \n",
      "\n",
      "epoch35, iter250, loss: 0.48190242052078247 \n",
      "\n",
      "epoch35, iter260, loss: 0.48349612951278687 \n",
      "\n",
      "epoch35, iter270, loss: 0.49871695041656494 \n",
      "\n",
      "epoch35, iter280, loss: 0.49359405040740967 \n",
      "\n",
      "epoch35, iter290, loss: 0.511375904083252 \n",
      "\n",
      "epoch35, iter300, loss: 0.6020111441612244 \n",
      "\n",
      "epoch35, iter310, loss: 0.4981096386909485 \n",
      "\n",
      "epoch35, iter320, loss: 0.5155214071273804 \n",
      "\n",
      "epoch35, iter330, loss: 0.4347043037414551 \n",
      "\n",
      "epoch35, iter340, loss: 0.44456690549850464 \n",
      "\n",
      "epoch35, iter350, loss: 0.4308586120605469 \n",
      "\n",
      "epoch35, iter360, loss: 0.4913097023963928 \n",
      "\n",
      "epoch35, iter370, loss: 0.5214370489120483 \n",
      "\n",
      "epoch35, iter380, loss: 0.46758145093917847 \n",
      "\n",
      "epoch35, iter390, loss: 0.48973560333251953 \n",
      "\n",
      "epoch35, iter400, loss: 0.5605476498603821 \n",
      "\n",
      "epoch35, iter410, loss: 0.5043045878410339 \n",
      "\n",
      "epoch35, iter420, loss: 0.5377312898635864 \n",
      "\n",
      "Finish epoch 35, time elapsed 1569.8901116847992 \n",
      "\n",
      "Training Check:\tLoss: 0.499344\tAccuracy: 50.192552\tIoU: 0.175208 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 299.600978\n",
      "Validation Results: Loss: 0.508963\tAccuracy: 49.240005\tIoU: 0.172041 \n",
      "\n",
      "epoch36, iter0, loss: 0.4594077467918396 \n",
      "\n",
      "epoch36, iter10, loss: 0.5865347385406494 \n",
      "\n",
      "epoch36, iter20, loss: 0.4353487491607666 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu:\n",
    "    fcn_model = fcn_model.to(device)\n",
    "    print(\"GPU works\")\n",
    "    \n",
    "best_loss = float('inf')\n",
    "prev_loss = float('inf')\n",
    "loss_inc_cnt = 0\n",
    "stop_early = False\n",
    "\n",
    "def train():\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    print(\"Starting Training\")\n",
    "    trn_losses = MovingAverage() \n",
    "    trn_accs = MovingAverage() \n",
    "    trn_ious = MovingAverage() \n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        ts = time.time()\n",
    "        for iter, (X, tar, Y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = X.to(device)\n",
    "                labels_cat = Y.to(device)\n",
    "                labels_enc = tar.to(device)\n",
    "            else:\n",
    "                inputs, labels_cat, labels_enc = X, Y, tar\n",
    "\n",
    "            outputs = softmax(fcn_model(inputs))\n",
    "            # labels_enc = torch.nn.functional.one_hot(labels_cat,num_classes =34).permute(0,3,1,2)\n",
    "            # print(labels_enc.size())\n",
    "            # print(outputs.size())\n",
    "            \n",
    "            # print(outputs.requires_grad)\n",
    "            # output_labels = F.one_hot(softmax(outputs).argmax(dim=1),num_classes=34).float()\n",
    "            # output_labels = F.one_hot(fcn_model(inputs).argmax(dim=1),num_classes=34).float()\n",
    "            # output_labels.requires_grad = True\n",
    "            # print(output_labels.requires_grad)\n",
    "            loss = dice_loss(outputs, labels_enc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "\n",
    "            output_labels = outputs.argmax(dim=1)\n",
    "\n",
    "            trn_losses.update(loss)\n",
    "            trn_accs.update(pixel_acc(output_labels, labels_cat))\n",
    "            trn_ious.update(np.nanmean(iou(output_labels, labels_cat)))\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print_info(\"epoch{}, iter{}, loss: {} \\n\".format(epoch, iter, loss))\n",
    "\n",
    "        print_info(\"Finish epoch {}, time elapsed {} \\n\".format(epoch, time.time() - ts))\n",
    "\n",
    "        loss, acc, IoU = trn_losses.value, trn_accs.value, trn_ious.value\n",
    "\n",
    "        print_info(\"Training Check:\\tLoss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "        \n",
    "        val(epoch)\n",
    "        if stop_early: return\n",
    "  \n",
    "def evaluate(data_loader, validation=False, verbose=False):\n",
    "\n",
    "    global best_loss\n",
    "    global prev_loss\n",
    "    global loss_inc_cnt\n",
    "    global stop_early\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accs = []\n",
    "        ious = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        ts = time.time()\n",
    "        print(\"Starting Evaluation\")\n",
    "        \n",
    "        for iter, (X, tar, Y) in enumerate(data_loader):\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs = X.to(device)\n",
    "                labels_cat = Y.to(device)\n",
    "                labels_enc = tar.to(device)\n",
    "            else:\n",
    "                inputs, labels_cat, labels_enc = X, Y, tar\n",
    "\n",
    "            outputs = softmax(fcn_model(inputs))\n",
    "            # labels_enc = torch.nn.functional.one_hot(labels_cat,num_classes =34).permute(0,3,1,2)\n",
    "            # output_labels = F.one_hot(outputs.argmax(dim=1),num_classes=34)\n",
    "\n",
    "            losses.append(dice_loss(outputs, labels_enc).item())\n",
    "\n",
    "            output_labels = outputs.argmax(dim=1)\n",
    "\n",
    "            accs.append(pixel_acc(output_labels, labels_cat))\n",
    "            \n",
    "            ious.append(np.nanmean(iou(output_labels, labels_cat)))\n",
    "  \n",
    "        print(\"Finished evaluation. Time elapsed %f\" % (time.time() - ts))\n",
    "\n",
    "        # This probably should not be a straight average, but just doing this for now\n",
    "        loss = np.mean(losses)\n",
    "        acc = np.mean(accs)\n",
    "        IoU = np.mean(ious)\n",
    "        \n",
    "        if validation:\n",
    "            if best_loss > loss:\n",
    "                best_loss = loss\n",
    "                print_info(\"Best Loss: \" + str(best_loss) + \"\\n\")\n",
    "                torch.save(fcn_model.state_dict(), best_model_fn)\n",
    "            loss_inc_cnt = loss_inc_cnt + 1 if prev_loss < loss else 0\n",
    "            if loss_inc_cnt > 3: stop_early = True\n",
    "            torch.save(fcn_model.state_dict(), model_fn)\n",
    "        \n",
    "        return loss, acc, IoU\n",
    "\n",
    "def val(epoch):\n",
    "    # fcn_model.eval()\n",
    "    # Complete this function - Calculate loss, accuracy and IoU for every epoch\n",
    "    # Make sure to include a softmax after the output from your model\n",
    "    loss, acc, IoU = evaluate(val_loader, validation=True)\n",
    "    print_info(\"Validation Results: Loss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "    if stop_early: print_info(\"Epoch %d:\\tStopping Early\" % (epoch))\n",
    "    \n",
    "def test():\n",
    "    print(' ')\n",
    "    # Complete this function - Calculate accuracy and IoU \n",
    "    # Make sure to include a softmax after the output from your model\n",
    "    loss, acc, IoU = evaluate(test_loader)\n",
    "    print_info(\"Test Results:\\tLoss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # val(0)  # show the accuracy before training\n",
    "    # print_info(\"---------Above is accuracy before training.---------\\n\")\n",
    "    train()\n",
    "    # test()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of base_dice.ipynb",
   "provenance": [
    {
     "file_id": "1CjKrUOG95bM0ENZwfsWEdPN6a5SsF_Vm",
     "timestamp": 1581707763471
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
