{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Test_Model_Eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMezdmzaY64P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tfC8Lii9vHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=avilaJorge&password=C323781C&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_B_LMPFPnmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=avilaJorge&password=C323781C&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_A7yUbJaCGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "dt_path = './datasets/cityscapes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o54sEbjsLaAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile('gtFine_trainvaltest.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(dt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrwKOhsJZX4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile('leftImg8bit_trainvaltest.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(dt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNQedZ0zsGnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLvzuas7mvfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import utils\n",
        "from dataloader import *\n",
        "from utils_2 import *\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import time\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ilKdSTsflaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCN8S(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class):\n",
        "        super(FCN8S, self).__init__()\n",
        "        self.n_class = n_class\n",
        "        self.relu    = nn.ReLU(inplace=False)\n",
        "        self.pool = nn.MaxPool2d(2, stride=2) \n",
        "        self.conv11 = nn.Conv2d(3,  64, kernel_size=3, stride=1, padding=100, dilation=1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(64,  128, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, padding=0)\n",
        "        self.dropout6 = nn.Dropout2d(p=0.5, inplace=True)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1, padding=0)\n",
        "        self.dropout7 = nn.Dropout(p=0.5, inplace=True)\n",
        "\n",
        "        self.clsfr1 = nn.Conv2d(4096, self.n_class, kernel_size=1, padding=0)\n",
        "        self.clsfr2 = nn.Conv2d(512, self.n_class, kernel_size=1, padding=0)\n",
        "        self.clsfr3 = nn.Conv2d(256, self.n_class, kernel_size=1, padding=0)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(self.n_class, self.n_class, kernel_size=4, stride=2, padding=0)\n",
        "        self.deconv2 = nn.ConvTranspose2d(self.n_class, self.n_class, kernel_size=4, stride=2, padding=0)\n",
        "        self.deconv3 = nn.ConvTranspose2d(self.n_class, self.n_class, kernel_size=16, stride=8, padding=0)\n",
        "\n",
        "    def __conv_relu(self, input, conv, pool=None):\n",
        "        if pool is None:\n",
        "          return self.relu(conv(input))\n",
        "        else:\n",
        "          return pool(self.relu(conv(input)))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.__conv_relu(x, self.conv11)\n",
        "        z = self.__conv_relu(z, self.conv12, self.pool)\n",
        "\n",
        "        z = self.__conv_relu(z, self.conv21)\n",
        "        z = self.__conv_relu(z, self.conv22, self.pool)\n",
        "\n",
        "        z = self.__conv_relu(z, self.conv31)\n",
        "        z = self.__conv_relu(z, self.conv32)\n",
        "        sp3 = self.__conv_relu(z, self.conv33, self.pool)\n",
        "\n",
        "        z = self.__conv_relu(sp3, self.conv41)\n",
        "        z = self.__conv_relu(z, self.conv42)\n",
        "        sp4 = self.__conv_relu(z, self.conv43, self.pool)\n",
        "\n",
        "        z = self.__conv_relu(sp4, self.conv51)\n",
        "        z = self.__conv_relu(z, self.conv52)\n",
        "        z = self.__conv_relu(z, self.conv53, self.pool)\n",
        "\n",
        "        z = self.dropout6(self.__conv_relu(z, self.conv6))\n",
        "\n",
        "        sfr = self.dropout7(self.__conv_relu(z, self.conv7))\n",
        "\n",
        "        ups2 = self.deconv1(self.clsfr1(sfr))\n",
        "        sp4  = self.clsfr2(sp4)\n",
        "        sp3  = self.clsfr3(sp3)\n",
        "        cropped = sp4[:, :, 10:(10 + ups2.size()[2]), 10:(10 + ups2.size()[3])]\n",
        "        ups4 = self.deconv2(sp4[:, :, 10:(10 + ups2.size()[2]), 10:(10 + ups2.size()[3])] + ups2)\n",
        "        ups3 = self.deconv3(sp3[:, :, 18:(18 + ups4.size()[2]), 18:(18 + ups4.size()[3])] + ups4)\n",
        "        score = ups3[:, :, 56: (56 + x.size(2)), 56: (56 + x.size(3))].contiguous()\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7pigYMMSnC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCN_bak(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_class):\n",
        "        super(FCN_bak, self).__init__()\n",
        "        self.n_class = n_class\n",
        "        self.conv1   = nn.Conv2d(3, 32, kernel_size=(3,5), stride=(2,4), padding=1, dilation=1)\n",
        "        self.bnd1    = nn.BatchNorm2d(32)\n",
        "        self.conv2   = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, dilation=1)\n",
        "        self.bnd2    = nn.BatchNorm2d(64)\n",
        "        self.conv3   = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, dilation=1)\n",
        "        self.bnd3    = nn.BatchNorm2d(128)\n",
        "        self.conv4   = nn.Conv2d(128,256, kernel_size=3, stride=2, padding=1, dilation=1)\n",
        "        self.bnd4    = nn.BatchNorm2d(256)\n",
        "        self.conv5   = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, dilation=1)\n",
        "        self.bnd5    = nn.BatchNorm2d(512)\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn1     = nn.BatchNorm2d(256)\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn2     = nn.BatchNorm2d(128)\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn3     = nn.BatchNorm2d(64)\n",
        "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn4     = nn.BatchNorm2d(32)\n",
        "        self.deconv5  = nn.ConvTranspose2d(32, 3, kernel_size=(3, 5), stride=(2,4), padding=1, dilation=1, output_padding=1)\n",
        "        self.bn5= nn.BatchNorm2d(3)\n",
        "        self.classifier = nn.Conv2d(3,n_class, kernel_size=1, stride=1, padding=0, dilation=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        pool = nn.MaxPool2d(2, stride=2,return_indices = True)\n",
        "        unpool = nn.MaxUnpool2d(2, stride=2)\n",
        "        \n",
        "        x1, indice1 = pool(self.relu(self.conv1(x)))\n",
        "        x2, indice2 = pool(self.relu(self.conv2(self.bnd1(x1))))\n",
        "        x3, indice3 = pool(self.relu(self.conv3(self.bnd2(x2))))\n",
        "        x4, indice4 = pool(self.relu(self.conv4(self.bnd3(x3))))\n",
        "        x5, indice5 = pool(self.relu(self.conv5(self.bnd4(x4))))\n",
        "        \n",
        "        z1 = self.deconv1(self.bnd5(self.relu(unpool((x5), indice5))))\n",
        "        z2 = self.deconv2(self.bn1(self.relu(unpool((z1), indice4))))\n",
        "        z3 = self.deconv3(self.bn2(self.relu(unpool((z2), indice3))))\n",
        "        z4 = self.deconv4(self.bn3(self.relu(unpool((z3), indice2))))\n",
        "        z5 = self.deconv5(self.bn4(self.relu(unpool((z4), indice1))))\n",
        "        \n",
        "        out_decoder = self.classifier(self.bn5(z5))                  \n",
        "\n",
        "        return out_decoder  # size=(N, n_class, x.H/1, x.W/1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPFL5PMtjH5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCN_TL(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_class):\n",
        "        super(FCN_TL, self).__init__()\n",
        "        self.n_class = n_class\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        # Parameters of newly constructed modules have requires_grad=True by default\n",
        "        self.model_conv = torchvision.models.resnet50(pretrained=True)\n",
        "        self.in_ftrs = 1000 \n",
        "        for param in self.model_conv.parameters():\n",
        "          param.requires_grad = False\n",
        "        \n",
        "        self.conv1   = nn.Conv2d(self.in_ftrs, 512, kernel_size=2, stride=1, padding=1, dilation=1)\n",
        "        self.cbnd1   = nn.BatchNorm2d(512)\n",
        "        self.conv2   = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=2, dilation=1)\n",
        "        self.cbnd2   = nn.BatchNorm2d(256)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn1     = nn.BatchNorm2d(128)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn2     = nn.BatchNorm2d(64)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn3     = nn.BatchNorm2d(32)\n",
        "        self.deconv4 = nn.ConvTranspose2d(32, 16, kernel_size=5, stride=4, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn4     = nn.BatchNorm2d(16)\n",
        "        self.deconv5 = nn.ConvTranspose2d(16, 8, kernel_size=5, stride=4, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn5     = nn.BatchNorm2d(8)\n",
        "        self.deconv6 = nn.ConvTranspose2d(8, 4, kernel_size=(3,5), stride=(2,4), padding=1, dilation=1, output_padding=1)\n",
        "        self.bn6     = nn.BatchNorm2d(4)\n",
        "        self.classifier = nn.Conv2d(4,self.n_class, kernel_size=1, stride=1, padding=0, dilation=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        z = self.model_conv(x).unsqueeze_(-1).unsqueeze_(-1)\n",
        "        z = self.cbnd1(self.relu(self.conv1(z)))\n",
        "        z = self.cbnd2(self.relu(self.conv2(z)))\n",
        "        z = self.bn1(self.relu(self.deconv1(z)))\n",
        "        z = self.bn2(self.relu(self.deconv2(z)))\n",
        "        z = self.bn3(self.relu(self.deconv3(z)))\n",
        "        z = self.bn4(self.relu(self.deconv4(z)))\n",
        "        z = self.bn5(self.relu(self.deconv5(z)))\n",
        "        z = self.bn6(self.relu(self.deconv6(z)))\n",
        "        \n",
        "        out_decoder = self.classifier(z)                  \n",
        "\n",
        "        return out_decoder  # size=(N, n_class, x.H/1, x.W/1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTEz6k_oTy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def imshow_test(img):\n",
        "#     H = img.size()[0]\n",
        "#     W = img.size()[1]\n",
        "#     test_colored = torch.zeros(H,W,3)\n",
        "#     for row in range(H):\n",
        "#       for col in range(W):\n",
        "#         test_colored[row][col][0] =  labels_classes[img[row][col]].color[0]\n",
        "#         test_colored[row][col][1] =  labels_classes[img[row][col]].color[1]\n",
        "#         test_colored[row][col][2] =  labels_classes[img[row][col]].color[2]\n",
        "#     print(img.size())   # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(npimg)\n",
        "#     plt.show()\n",
        "\n",
        "def imshow_test(img, original, fn):\n",
        "    H = int(img.size()[0])\n",
        "    W = int(img.size()[1])\n",
        "    test_colored = np.zeros((H,W,3),dtype=np.uint8)\n",
        "\n",
        "    for row in range(H):\n",
        "      for col in range(W):\n",
        "        test_colored[row][col][0] =  labels_classes[img[row][col]].color[0]\n",
        "        test_colored[row][col][1] =  labels_classes[img[row][col]].color[1]\n",
        "        test_colored[row][col][2] =  labels_classes[img[row][col]].color[2]\n",
        "\n",
        "    test = Image.fromarray(test_colored)\n",
        "    blended = Image.blend(original, test, alpha=0.5)#change alpha will change the transparency\n",
        "    display(blended)\n",
        "    blended.save(fn)\n",
        "\n",
        "def test_img(data_loader, mdl):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        ts = time.time()\n",
        "        outputs = []\n",
        "        output_labels = []\n",
        "        print(\"Starting Testing\")\n",
        "        \n",
        "        for iter, (X, tar, Y) in enumerate(data_loader):\n",
        "\n",
        "            inputs = X.to(device)\n",
        "            labels_cat = Y.to(device)\n",
        "\n",
        "            outputs = softmax(mdl(inputs))\n",
        "\n",
        "            output_labels = outputs.argmax(dim=1)\n",
        "\n",
        "        print(\"Finished evaluation. Time elapsed %f\\n\" % (time.time() - ts))\n",
        "        return outputs, output_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LRJxaYdvceD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_wrkrs = 4\n",
        "test_dataset_resized = CityScapesDatasetResize(csv_file='img_drive.csv')\n",
        "test_dataset = CityScapesDataset(csv_file='img_drive.csv')\n",
        "test_resized_loader = DataLoader(dataset=test_dataset_resized,\n",
        "                          batch_size=1,\n",
        "                          num_workers=num_wrkrs,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                          batch_size=1,\n",
        "                          num_workers=num_wrkrs,\n",
        "                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rcc90kGoNqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = CityScapesValidationDataset(csv_file='val_local.csv')\n",
        "val_loader = DataLoader(dataset=val_dataset,\n",
        "                          batch_size=4,\n",
        "                          num_workers=num_wrkrs,\n",
        "                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTFTczwShUxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_loader, mdl, scale=False):\n",
        "    rprt_ids = {\n",
        "      'building': [],\n",
        "      'traffic sign': [],\n",
        "      'person': [],\n",
        "      'car': [],\n",
        "      'bicycle': []\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        for iter, (X, tar, Y) in enumerate(data_loader):\n",
        "\n",
        "            inputs = X.to(device)\n",
        "            labels_cat = Y.to(device)\n",
        "\n",
        "            outputs = softmax(mdl(inputs))\n",
        "            if scale:\n",
        "              outputs = torch.nn.functional.interpolate(input=outputs, size=(1024,2048))\n",
        "\n",
        "            output_labels = outputs.argmax(dim=1)\n",
        "\n",
        "            ious, rprt = iou(output_labels, labels_cat)\n",
        "            print(rprt)\n",
        "            for cls in rprt.keys():\n",
        "              rprt_ids[cls].append(rprt[cls])\n",
        "    return rprt_ids\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "nu7sqQI8SnDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "def print_GPU_stats():\n",
        "    print(\"total GPU Mem: \", torch.cuda.get_device_properties(device).total_memory)\n",
        "    print(\"total GPU Cached: \", torch.cuda.memory_cached(device))\n",
        "    print(\"total GPU Allocated: \", torch.cuda.memory_allocated(device))\n",
        "    print(\"Available GB: \", (torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device))/(10**9))\n",
        "print_GPU_stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6F9NuBMgEyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "original_image = Image.open(\"berlin_000000_000019_leftImg8bit.png\")\n",
        "epochs     = 100\n",
        "start_epoch = 1\n",
        "path = '/content/drive/My Drive/CSE253/PA3/FCN8S/'\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "fcn_model = FCN8S(n_class=34)\n",
        "\n",
        "fcn_model.load_state_dict(torch.load(path + 'best_model_02_16_07_22.pt'))\n",
        "fcn_model = fcn_model.to(device)\n",
        "optimizer = optim.Adam(fcn_model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "\n",
        "imgs, _, labels = next(iter(test_loader))\n",
        "imgs_resized, _, labels_resized = next(iter(test_resized_loader))\n",
        "\n",
        "# imshow_test(labels.squeeze(0))\n",
        "\n",
        "# img, lbls = test_img(test_loader)\n",
        "# imshow_test(lbls.squeeze(0).cpu())\n",
        "\n",
        "# resized_img, resized_lbls = test_img(test_resized_loader, fcn_model)\n",
        "# imshow_test(resized_lbls.squeeze(0).cpu())\n",
        "# print(resized_lbls.size())\n",
        "# ups_img = torch.nn.functional.interpolate(input=resized_lbls.unsqueeze(0).float(), size=(1024, 2048))\n",
        "# print(ups_img.size())\n",
        "# imshow_test(ups_img.squeeze(0).squeeze().long().cpu(), original_image, 'FCN8S.png')\n",
        "rprt = evaluate(val_loader, fcn_model, scale=True)\n",
        "for ky in rprt.keys():\n",
        "  print(ky, np.nanmean(rprt[ky]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cne5pF2X1Bba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset_pln = CityScapesDataset(csv_file='val_local.csv')\n",
        "val_ldr_pln = DataLoader(dataset=val_dataset_pln,\n",
        "                          batch_size=4,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aw7W8NIrceMk",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "original_image = Image.open(\"berlin_000000_000019_leftImg8bit.png\")\n",
        "epochs     = 100\n",
        "start_epoch = 1\n",
        "path = '/content/drive/My Drive/CSE253/PA3/Base_TL/'\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "fcn_model_TL = FCN_TL(n_class=34)\n",
        "\n",
        "fcn_model_TL.load_state_dict(torch.load(path + 'best_model_02_15_21_27.pt'))\n",
        "fcn_model_TL = fcn_model_TL.to(device)\n",
        "optimizer = optim.Adam(fcn_model_TL.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "\n",
        "imgs, _, labels = next(iter(test_loader))\n",
        "\n",
        "img, lbls = test_img(test_loader, fcn_model_TL)\n",
        "print(lbls.size())\n",
        "imshow_test(lbls.squeeze(0).long().cpu(), original_image, 'FCN_TL.png')\n",
        "rprt = evaluate(val_ldr_pln, fcn_model_TL)\n",
        "for ky in rprt.keys():\n",
        "  print(ky, np.nanmean(rprt[ky]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDiPzFnHygZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "original_image = Image.open(\"berlin_000000_000019_leftImg8bit.png\")\n",
        "epochs     = 100\n",
        "start_epoch = 1\n",
        "path = '/content/drive/My Drive/CSE253/PA3/Base_LR00005/'\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "fcn_model_bs = FCN_bak(n_class=34)\n",
        "\n",
        "fcn_model_bs.load_state_dict(torch.load(path + 'best_model_02_14_11_16.pt'))\n",
        "fcn_model_bs = fcn_model_bs.to(device)\n",
        "optimizer = optim.Adam(fcn_model_bs.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "\n",
        "imgs, _, labels = next(iter(test_loader))\n",
        "\n",
        "img, lbls = test_img(test_loader, fcn_model_bs)\n",
        "print(lbls.size())\n",
        "imshow_test(lbls.squeeze(0).long().cpu(), original_image, 'FCN_Base.png')\n",
        "rprt = evaluate(val_ldr_pln, fcn_model_bs)\n",
        "for ky in rprt.keys():\n",
        "  print(ky, np.nanmean(rprt[ky]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}