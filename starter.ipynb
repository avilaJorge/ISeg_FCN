{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import time\n",
    "from basic_fcn import FCN\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name to FCN to use this model instead I think\n",
    "\n",
    "class FCN_bak(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(FCN_bak, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.conv1   = nn.Conv2d(3, 32, kernel_size=(3,5), stride=(2,4), padding=1, dilation=1)\n",
    "        self.bnd1    = nn.BatchNorm2d(32)\n",
    "        self.conv2   = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd2    = nn.BatchNorm2d(64)\n",
    "        self.conv3   = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd3    = nn.BatchNorm2d(128)\n",
    "        self.conv4   = nn.Conv2d(128,256, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd4    = nn.BatchNorm2d(256)\n",
    "        self.conv5   = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        self.bnd5    = nn.BatchNorm2d(512)\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(256)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(128)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(64)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(32)\n",
    "        self.deconv5  = nn.ConvTranspose2d(32, 3, kernel_size=(3, 5), stride=(2,4), padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5= nn.BatchNorm2d(3)\n",
    "        self.classifier = nn.Conv2d(3,n_class, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pool = nn.MaxPool2d(2, stride=2,return_indices = True)\n",
    "        unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "        \n",
    "        x1, indice1 = pool(self.relu(self.conv1(x)))\n",
    "        x2, indice2 = pool(self.relu(self.conv2(self.bnd1(x1))))\n",
    "        x3, indice3 = pool(self.relu(self.conv3(self.bnd2(x2))))\n",
    "        x4, indice4 = pool(self.relu(self.conv4(self.bnd3(x3))))\n",
    "        x5, indice5 = pool(self.relu(self.conv5(self.bnd4(x4))))\n",
    "        \n",
    "        z1 = self.deconv1(self.bnd5(self.relu(unpool((x5), indice5))))\n",
    "        z2 = self.deconv2(self.bn1(self.relu(unpool((z1), indice4))))\n",
    "        z3 = self.deconv3(self.bn2(self.relu(unpool((z2), indice3))))\n",
    "        z4 = self.deconv4(self.bn3(self.relu(unpool((z3), indice2))))\n",
    "        z5 = self.deconv5(self.bn4(self.relu(unpool((z4), indice1))))\n",
    "        \n",
    "        out_decoder = self.classifier(self.bn5(z5))                  \n",
    "\n",
    "        return out_decoder  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "total GPU Mem:  11721506816\n",
      "total GPU Cached:  0\n",
      "total GPU Allocated:  0\n",
      "Available GB:  11.721506816\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "def print_GPU_stats():\n",
    "    print(\"total GPU Mem: \", torch.cuda.get_device_properties(device).total_memory)\n",
    "    print(\"total GPU Cached: \", torch.cuda.memory_cached(device))\n",
    "    print(\"total GPU Allocated: \", torch.cuda.memory_allocated(device))\n",
    "    print(\"Available GB: \", (torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device))/(10**9))\n",
    "print_GPU_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dataset = CityScapesDataset(csv_file='train.csv')\n",
    "val_dataset = CityScapesDataset(csv_file='val.csv')\n",
    "test_dataset = CityScapesDataset(csv_file='test.csv')\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=6,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=6,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=6,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "#         torch.nn.init.xavier_uniform(m.bias.data)\n",
    "        m.bias.data.zero_()\n",
    "epochs     = 100\n",
    "start_epoch = 9\n",
    "#criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "fcn_model = FCN_bak(n_class=34)\n",
    "# fcn_model.apply(init_weights)\n",
    "fcn_model.load_state_dict(torch.load('best_model_02_10_22_04.pt'))\n",
    "optimizer = optim.Adam(fcn_model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 2020-02-11 03:50:05.235928\n",
      "From a previously trained model which left off on start of epoch 9.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "output_fn = \"model_output_\" + dt + \".txt\"\n",
    "best_model_fn = \"best_model_\" + dt + \".pt\"\n",
    "\n",
    "def print_info(out_str):\n",
    "    f = open(output_fn,\"a\")\n",
    "    print(out_str)\n",
    "    f.write(out_str)\n",
    "    f.close()\n",
    "\n",
    "print_info(\"Started: %s\\nFrom a previously trained model which left off on start of epoch 9.\\n\" % datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch9, iter0, loss: 2.9501595497131348 \n",
      "\n",
      "epoch9, iter10, loss: 2.941798686981201 \n",
      "\n",
      "epoch9, iter20, loss: 2.965298652648926 \n",
      "\n",
      "epoch9, iter30, loss: 3.005303382873535 \n",
      "\n",
      "epoch9, iter40, loss: 3.0456883907318115 \n",
      "\n",
      "epoch9, iter50, loss: 2.896493673324585 \n",
      "\n",
      "epoch9, iter60, loss: 2.9909722805023193 \n",
      "\n",
      "epoch9, iter70, loss: 2.9460575580596924 \n",
      "\n",
      "epoch9, iter80, loss: 3.0054445266723633 \n",
      "\n",
      "epoch9, iter90, loss: 3.000957727432251 \n",
      "\n",
      "epoch9, iter100, loss: 2.9923863410949707 \n",
      "\n",
      "epoch9, iter110, loss: 3.0024466514587402 \n",
      "\n",
      "epoch9, iter120, loss: 3.05564546585083 \n",
      "\n",
      "epoch9, iter130, loss: 3.0212368965148926 \n",
      "\n",
      "epoch9, iter140, loss: 2.936124801635742 \n",
      "\n",
      "epoch9, iter150, loss: 2.988154172897339 \n",
      "\n",
      "epoch9, iter160, loss: 2.955958843231201 \n",
      "\n",
      "epoch9, iter170, loss: 2.9196255207061768 \n",
      "\n",
      "epoch9, iter180, loss: 2.9580237865448 \n",
      "\n",
      "epoch9, iter190, loss: 3.0395750999450684 \n",
      "\n",
      "epoch9, iter200, loss: 3.017932891845703 \n",
      "\n",
      "epoch9, iter210, loss: 3.0255093574523926 \n",
      "\n",
      "epoch9, iter220, loss: 2.978437662124634 \n",
      "\n",
      "epoch9, iter230, loss: 2.9697928428649902 \n",
      "\n",
      "epoch9, iter240, loss: 2.9308598041534424 \n",
      "\n",
      "epoch9, iter250, loss: 2.9572389125823975 \n",
      "\n",
      "epoch9, iter260, loss: 2.9944117069244385 \n",
      "\n",
      "epoch9, iter270, loss: 2.9930710792541504 \n",
      "\n",
      "epoch9, iter280, loss: 3.0197465419769287 \n",
      "\n",
      "epoch9, iter290, loss: 2.934746503829956 \n",
      "\n",
      "epoch9, iter300, loss: 3.0199804306030273 \n",
      "\n",
      "epoch9, iter310, loss: 2.9703564643859863 \n",
      "\n",
      "epoch9, iter320, loss: 2.9850034713745117 \n",
      "\n",
      "epoch9, iter330, loss: 2.954235792160034 \n",
      "\n",
      "epoch9, iter340, loss: 2.972062826156616 \n",
      "\n",
      "epoch9, iter350, loss: 2.9905104637145996 \n",
      "\n",
      "epoch9, iter360, loss: 2.9629688262939453 \n",
      "\n",
      "epoch9, iter370, loss: 2.945884943008423 \n",
      "\n",
      "epoch9, iter380, loss: 3.0051016807556152 \n",
      "\n",
      "epoch9, iter390, loss: 2.9753427505493164 \n",
      "\n",
      "epoch9, iter400, loss: 3.0197834968566895 \n",
      "\n",
      "epoch9, iter410, loss: 3.0570321083068848 \n",
      "\n",
      "epoch9, iter420, loss: 2.9519498348236084 \n",
      "\n",
      "epoch9, iter430, loss: 3.0048553943634033 \n",
      "\n",
      "epoch9, iter440, loss: 2.9498019218444824 \n",
      "\n",
      "epoch9, iter450, loss: 2.986475706100464 \n",
      "\n",
      "epoch9, iter460, loss: 3.0202460289001465 \n",
      "\n",
      "epoch9, iter470, loss: 3.022111415863037 \n",
      "\n",
      "epoch9, iter480, loss: 2.9335973262786865 \n",
      "\n",
      "epoch9, iter490, loss: 2.9985268115997314 \n",
      "\n",
      "epoch9, iter500, loss: 2.9814581871032715 \n",
      "\n",
      "epoch9, iter510, loss: 2.9314308166503906 \n",
      "\n",
      "epoch9, iter520, loss: 2.931514024734497 \n",
      "\n",
      "epoch9, iter530, loss: 2.992124319076538 \n",
      "\n",
      "epoch9, iter540, loss: 2.9824438095092773 \n",
      "\n",
      "epoch9, iter550, loss: 2.9459691047668457 \n",
      "\n",
      "epoch9, iter560, loss: 3.013244867324829 \n",
      "\n",
      "epoch9, iter570, loss: 2.961010694503784 \n",
      "\n",
      "epoch9, iter580, loss: 3.029252529144287 \n",
      "\n",
      "epoch9, iter590, loss: 2.9306647777557373 \n",
      "\n",
      "Finish epoch 9, time elapsed 1371.6312148571014 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1431.136889\n",
      "Training Check:\tLoss: 2.980454\tAccuracy: 59.581039\tIoU: 0.067211 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 244.318215\n",
      "Best Loss: 2.986284306049347\n",
      "\n",
      "Validation Results: Loss: 2.986284\tAccuracy: 58.992758\tIoU: 0.065264 \n",
      "\n",
      "epoch10, iter0, loss: 3.023634433746338 \n",
      "\n",
      "epoch10, iter10, loss: 2.9658615589141846 \n",
      "\n",
      "epoch10, iter20, loss: 2.935621500015259 \n",
      "\n",
      "epoch10, iter30, loss: 2.978062629699707 \n",
      "\n",
      "epoch10, iter40, loss: 2.986733913421631 \n",
      "\n",
      "epoch10, iter50, loss: 2.947913646697998 \n",
      "\n",
      "epoch10, iter60, loss: 2.941476583480835 \n",
      "\n",
      "epoch10, iter70, loss: 2.9596428871154785 \n",
      "\n",
      "epoch10, iter80, loss: 2.941162109375 \n",
      "\n",
      "epoch10, iter90, loss: 3.0540597438812256 \n",
      "\n",
      "epoch10, iter100, loss: 2.942429780960083 \n",
      "\n",
      "epoch10, iter110, loss: 2.9753944873809814 \n",
      "\n",
      "epoch10, iter120, loss: 3.0155367851257324 \n",
      "\n",
      "epoch10, iter130, loss: 3.016119956970215 \n",
      "\n",
      "epoch10, iter140, loss: 2.9981088638305664 \n",
      "\n",
      "epoch10, iter150, loss: 2.942688226699829 \n",
      "\n",
      "epoch10, iter160, loss: 2.930891990661621 \n",
      "\n",
      "epoch10, iter170, loss: 2.9399502277374268 \n",
      "\n",
      "epoch10, iter180, loss: 2.9750850200653076 \n",
      "\n",
      "epoch10, iter190, loss: 2.9366440773010254 \n",
      "\n",
      "epoch10, iter200, loss: 3.0041990280151367 \n",
      "\n",
      "epoch10, iter210, loss: 3.0024075508117676 \n",
      "\n",
      "epoch10, iter220, loss: 2.937185287475586 \n",
      "\n",
      "epoch10, iter230, loss: 2.944612979888916 \n",
      "\n",
      "epoch10, iter240, loss: 2.9166512489318848 \n",
      "\n",
      "epoch10, iter250, loss: 2.980438709259033 \n",
      "\n",
      "epoch10, iter260, loss: 3.0621585845947266 \n",
      "\n",
      "epoch10, iter270, loss: 3.0115113258361816 \n",
      "\n",
      "epoch10, iter280, loss: 2.9322688579559326 \n",
      "\n",
      "epoch10, iter290, loss: 2.9533934593200684 \n",
      "\n",
      "epoch10, iter300, loss: 2.952265977859497 \n",
      "\n",
      "epoch10, iter310, loss: 2.999001979827881 \n",
      "\n",
      "epoch10, iter320, loss: 2.9801900386810303 \n",
      "\n",
      "epoch10, iter330, loss: 2.9550600051879883 \n",
      "\n",
      "epoch10, iter340, loss: 2.968374013900757 \n",
      "\n",
      "epoch10, iter350, loss: 2.9426064491271973 \n",
      "\n",
      "epoch10, iter360, loss: 2.895068645477295 \n",
      "\n",
      "epoch10, iter370, loss: 3.0121331214904785 \n",
      "\n",
      "epoch10, iter380, loss: 2.9863224029541016 \n",
      "\n",
      "epoch10, iter390, loss: 2.953418731689453 \n",
      "\n",
      "epoch10, iter400, loss: 2.9658150672912598 \n",
      "\n",
      "epoch10, iter410, loss: 2.9702470302581787 \n",
      "\n",
      "epoch10, iter420, loss: 2.9833600521087646 \n",
      "\n",
      "epoch10, iter430, loss: 2.9679479598999023 \n",
      "\n",
      "epoch10, iter440, loss: 2.947878122329712 \n",
      "\n",
      "epoch10, iter450, loss: 3.0490832328796387 \n",
      "\n",
      "epoch10, iter460, loss: 2.9466466903686523 \n",
      "\n",
      "epoch10, iter470, loss: 3.005519390106201 \n",
      "\n",
      "epoch10, iter480, loss: 3.1442813873291016 \n",
      "\n",
      "epoch10, iter490, loss: 2.9615511894226074 \n",
      "\n",
      "epoch10, iter500, loss: 2.9843647480010986 \n",
      "\n",
      "epoch10, iter510, loss: 2.98392915725708 \n",
      "\n",
      "epoch10, iter520, loss: 2.991971492767334 \n",
      "\n",
      "epoch10, iter530, loss: 2.9847989082336426 \n",
      "\n",
      "epoch10, iter540, loss: 2.9873416423797607 \n",
      "\n",
      "epoch10, iter550, loss: 2.9961788654327393 \n",
      "\n",
      "epoch10, iter560, loss: 3.020761728286743 \n",
      "\n",
      "epoch10, iter570, loss: 2.9668312072753906 \n",
      "\n",
      "epoch10, iter580, loss: 2.95812726020813 \n",
      "\n",
      "epoch10, iter590, loss: 2.963203191757202 \n",
      "\n",
      "Finish epoch 10, time elapsed 1367.2054653167725 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1443.449003\n",
      "Training Check:\tLoss: 2.976674\tAccuracy: 59.933106\tIoU: 0.068139 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 244.276378\n",
      "Best Loss: 2.982817280292511\n",
      "\n",
      "Validation Results: Loss: 2.982817\tAccuracy: 59.317392\tIoU: 0.066063 \n",
      "\n",
      "epoch11, iter0, loss: 3.087967872619629 \n",
      "\n",
      "epoch11, iter10, loss: 2.9566025733947754 \n",
      "\n",
      "epoch11, iter20, loss: 2.9487903118133545 \n",
      "\n",
      "epoch11, iter30, loss: 2.988826274871826 \n",
      "\n",
      "epoch11, iter40, loss: 2.900726795196533 \n",
      "\n",
      "epoch11, iter50, loss: 3.0326051712036133 \n",
      "\n",
      "epoch11, iter60, loss: 2.929569721221924 \n",
      "\n",
      "epoch11, iter70, loss: 3.0193657875061035 \n",
      "\n",
      "epoch11, iter80, loss: 2.9435229301452637 \n",
      "\n",
      "epoch11, iter90, loss: 2.961256504058838 \n",
      "\n",
      "epoch11, iter100, loss: 2.9740076065063477 \n",
      "\n",
      "epoch11, iter110, loss: 3.0238919258117676 \n",
      "\n",
      "epoch11, iter120, loss: 2.934325933456421 \n",
      "\n",
      "epoch11, iter130, loss: 2.9335732460021973 \n",
      "\n",
      "epoch11, iter140, loss: 2.941152334213257 \n",
      "\n",
      "epoch11, iter150, loss: 3.013993740081787 \n",
      "\n",
      "epoch11, iter160, loss: 2.937333583831787 \n",
      "\n",
      "epoch11, iter170, loss: 2.968414306640625 \n",
      "\n",
      "epoch11, iter180, loss: 3.0231785774230957 \n",
      "\n",
      "epoch11, iter190, loss: 3.0005087852478027 \n",
      "\n",
      "epoch11, iter200, loss: 2.971752643585205 \n",
      "\n",
      "epoch11, iter210, loss: 3.0202250480651855 \n",
      "\n",
      "epoch11, iter220, loss: 2.968271255493164 \n",
      "\n",
      "epoch11, iter230, loss: 2.952785015106201 \n",
      "\n",
      "epoch11, iter240, loss: 2.9859392642974854 \n",
      "\n",
      "epoch11, iter250, loss: 2.9786224365234375 \n",
      "\n",
      "epoch11, iter260, loss: 3.122014284133911 \n",
      "\n",
      "epoch11, iter270, loss: 2.944174289703369 \n",
      "\n",
      "epoch11, iter280, loss: 2.9097018241882324 \n",
      "\n",
      "epoch11, iter290, loss: 2.9861414432525635 \n",
      "\n",
      "epoch11, iter300, loss: 2.9383797645568848 \n",
      "\n",
      "epoch11, iter310, loss: 2.956550121307373 \n",
      "\n",
      "epoch11, iter320, loss: 3.019406318664551 \n",
      "\n",
      "epoch11, iter330, loss: 2.9942009449005127 \n",
      "\n",
      "epoch11, iter340, loss: 2.9740371704101562 \n",
      "\n",
      "epoch11, iter350, loss: 2.990449905395508 \n",
      "\n",
      "epoch11, iter360, loss: 2.9941039085388184 \n",
      "\n",
      "epoch11, iter370, loss: 3.000901460647583 \n",
      "\n",
      "epoch11, iter380, loss: 2.991136074066162 \n",
      "\n",
      "epoch11, iter390, loss: 3.0044989585876465 \n",
      "\n",
      "epoch11, iter400, loss: 2.9313085079193115 \n",
      "\n",
      "epoch11, iter410, loss: 2.979865550994873 \n",
      "\n",
      "epoch11, iter420, loss: 2.9443392753601074 \n",
      "\n",
      "epoch11, iter430, loss: 3.0021586418151855 \n",
      "\n",
      "epoch11, iter440, loss: 2.9644837379455566 \n",
      "\n",
      "epoch11, iter450, loss: 2.9617700576782227 \n",
      "\n",
      "epoch11, iter460, loss: 2.942966938018799 \n",
      "\n",
      "epoch11, iter470, loss: 2.9465274810791016 \n",
      "\n",
      "epoch11, iter480, loss: 2.9767706394195557 \n",
      "\n",
      "epoch11, iter490, loss: 2.9633874893188477 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch11, iter500, loss: 2.9576776027679443 \n",
      "\n",
      "epoch11, iter510, loss: 2.9894256591796875 \n",
      "\n",
      "epoch11, iter520, loss: 2.933023452758789 \n",
      "\n",
      "epoch11, iter530, loss: 2.8976430892944336 \n",
      "\n",
      "epoch11, iter540, loss: 2.9031898975372314 \n",
      "\n",
      "epoch11, iter550, loss: 2.935774564743042 \n",
      "\n",
      "epoch11, iter560, loss: 2.885760545730591 \n",
      "\n",
      "epoch11, iter570, loss: 2.9440040588378906 \n",
      "\n",
      "epoch11, iter580, loss: 2.9593465328216553 \n",
      "\n",
      "epoch11, iter590, loss: 3.0178024768829346 \n",
      "\n",
      "Finish epoch 11, time elapsed 1366.168613910675 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1436.008109\n",
      "Training Check:\tLoss: 2.965637\tAccuracy: 61.045176\tIoU: 0.069857 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 247.090788\n",
      "Best Loss: 2.971919884681702\n",
      "\n",
      "Validation Results: Loss: 2.971920\tAccuracy: 60.417435\tIoU: 0.067947 \n",
      "\n",
      "epoch12, iter0, loss: 3.0201528072357178 \n",
      "\n",
      "epoch12, iter10, loss: 2.9419302940368652 \n",
      "\n",
      "epoch12, iter20, loss: 3.0145716667175293 \n",
      "\n",
      "epoch12, iter30, loss: 2.897193670272827 \n",
      "\n",
      "epoch12, iter40, loss: 2.9515469074249268 \n",
      "\n",
      "epoch12, iter50, loss: 2.9442026615142822 \n",
      "\n",
      "epoch12, iter60, loss: 3.000974655151367 \n",
      "\n",
      "epoch12, iter70, loss: 3.0402772426605225 \n",
      "\n",
      "epoch12, iter80, loss: 2.9350574016571045 \n",
      "\n",
      "epoch12, iter90, loss: 2.9580368995666504 \n",
      "\n",
      "epoch12, iter100, loss: 2.908176898956299 \n",
      "\n",
      "epoch12, iter110, loss: 2.9650516510009766 \n",
      "\n",
      "epoch12, iter120, loss: 2.9675240516662598 \n",
      "\n",
      "epoch12, iter130, loss: 2.971876382827759 \n",
      "\n",
      "epoch12, iter140, loss: 3.031973123550415 \n",
      "\n",
      "epoch12, iter150, loss: 2.9879109859466553 \n",
      "\n",
      "epoch12, iter160, loss: 2.9807052612304688 \n",
      "\n",
      "epoch12, iter170, loss: 3.0445218086242676 \n",
      "\n",
      "epoch12, iter180, loss: 2.995593547821045 \n",
      "\n",
      "epoch12, iter190, loss: 2.9848036766052246 \n",
      "\n",
      "epoch12, iter200, loss: 2.991189479827881 \n",
      "\n",
      "epoch12, iter210, loss: 2.989531993865967 \n",
      "\n",
      "epoch12, iter220, loss: 2.978762149810791 \n",
      "\n",
      "epoch12, iter230, loss: 2.9134747982025146 \n",
      "\n",
      "epoch12, iter240, loss: 3.0442581176757812 \n",
      "\n",
      "epoch12, iter250, loss: 2.92960786819458 \n",
      "\n",
      "epoch12, iter260, loss: 3.023362874984741 \n",
      "\n",
      "epoch12, iter270, loss: 2.976010799407959 \n",
      "\n",
      "epoch12, iter280, loss: 3.0629494190216064 \n",
      "\n",
      "epoch12, iter290, loss: 3.0110507011413574 \n",
      "\n",
      "epoch12, iter300, loss: 3.0416221618652344 \n",
      "\n",
      "epoch12, iter310, loss: 2.916531801223755 \n",
      "\n",
      "epoch12, iter320, loss: 2.9434714317321777 \n",
      "\n",
      "epoch12, iter330, loss: 2.9926257133483887 \n",
      "\n",
      "epoch12, iter340, loss: 2.980762004852295 \n",
      "\n",
      "epoch12, iter350, loss: 2.9600906372070312 \n",
      "\n",
      "epoch12, iter360, loss: 2.9465813636779785 \n",
      "\n",
      "epoch12, iter370, loss: 3.026945114135742 \n",
      "\n",
      "epoch12, iter380, loss: 2.9699032306671143 \n",
      "\n",
      "epoch12, iter390, loss: 2.97361159324646 \n",
      "\n",
      "epoch12, iter400, loss: 2.9508397579193115 \n",
      "\n",
      "epoch12, iter410, loss: 3.0726799964904785 \n",
      "\n",
      "epoch12, iter420, loss: 2.9472527503967285 \n",
      "\n",
      "epoch12, iter430, loss: 2.9541404247283936 \n",
      "\n",
      "epoch12, iter440, loss: 2.975064992904663 \n",
      "\n",
      "epoch12, iter450, loss: 2.9589250087738037 \n",
      "\n",
      "epoch12, iter460, loss: 2.9732367992401123 \n",
      "\n",
      "epoch12, iter470, loss: 3.0602707862854004 \n",
      "\n",
      "epoch12, iter480, loss: 2.9983909130096436 \n",
      "\n",
      "epoch12, iter490, loss: 3.0117502212524414 \n",
      "\n",
      "epoch12, iter500, loss: 2.9692845344543457 \n",
      "\n",
      "epoch12, iter510, loss: 2.981464147567749 \n",
      "\n",
      "epoch12, iter520, loss: 2.9403254985809326 \n",
      "\n",
      "epoch12, iter530, loss: 2.9190354347229004 \n",
      "\n",
      "epoch12, iter540, loss: 2.9294509887695312 \n",
      "\n",
      "epoch12, iter550, loss: 2.907503604888916 \n",
      "\n",
      "epoch12, iter560, loss: 2.9896254539489746 \n",
      "\n",
      "epoch12, iter570, loss: 2.9548611640930176 \n",
      "\n",
      "epoch12, iter580, loss: 3.0035128593444824 \n",
      "\n",
      "epoch12, iter590, loss: 2.9407482147216797 \n",
      "\n",
      "Finish epoch 12, time elapsed 1368.085612297058 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1443.773378\n",
      "Training Check:\tLoss: 2.976098\tAccuracy: 60.021136\tIoU: 0.068345 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 247.251794\n",
      "Validation Results: Loss: 2.981784\tAccuracy: 59.447361\tIoU: 0.066700 \n",
      "\n",
      "epoch13, iter0, loss: 2.99981689453125 \n",
      "\n",
      "epoch13, iter10, loss: 2.979640245437622 \n",
      "\n",
      "epoch13, iter20, loss: 2.969515323638916 \n",
      "\n",
      "epoch13, iter30, loss: 2.9544854164123535 \n",
      "\n",
      "epoch13, iter40, loss: 2.9245145320892334 \n",
      "\n",
      "epoch13, iter50, loss: 2.9878861904144287 \n",
      "\n",
      "epoch13, iter60, loss: 2.9333889484405518 \n",
      "\n",
      "epoch13, iter70, loss: 2.967146873474121 \n",
      "\n",
      "epoch13, iter80, loss: 2.9422099590301514 \n",
      "\n",
      "epoch13, iter90, loss: 2.9690492153167725 \n",
      "\n",
      "epoch13, iter100, loss: 2.9580752849578857 \n",
      "\n",
      "epoch13, iter110, loss: 2.984443187713623 \n",
      "\n",
      "epoch13, iter120, loss: 2.9671690464019775 \n",
      "\n",
      "epoch13, iter130, loss: 2.9136452674865723 \n",
      "\n",
      "epoch13, iter140, loss: 2.991344690322876 \n",
      "\n",
      "epoch13, iter150, loss: 3.014348268508911 \n",
      "\n",
      "epoch13, iter160, loss: 2.9903957843780518 \n",
      "\n",
      "epoch13, iter170, loss: 2.9840810298919678 \n",
      "\n",
      "epoch13, iter180, loss: 3.0265371799468994 \n",
      "\n",
      "epoch13, iter190, loss: 2.94994854927063 \n",
      "\n",
      "epoch13, iter200, loss: 2.980525016784668 \n",
      "\n",
      "epoch13, iter210, loss: 2.953598737716675 \n",
      "\n",
      "epoch13, iter220, loss: 2.964416742324829 \n",
      "\n",
      "epoch13, iter230, loss: 3.028266191482544 \n",
      "\n",
      "epoch13, iter240, loss: 2.9761996269226074 \n",
      "\n",
      "epoch13, iter250, loss: 2.950024127960205 \n",
      "\n",
      "epoch13, iter260, loss: 3.0349271297454834 \n",
      "\n",
      "epoch13, iter270, loss: 2.932568311691284 \n",
      "\n",
      "epoch13, iter280, loss: 2.9921748638153076 \n",
      "\n",
      "epoch13, iter290, loss: 2.912386178970337 \n",
      "\n",
      "epoch13, iter300, loss: 2.937849283218384 \n",
      "\n",
      "epoch13, iter310, loss: 2.978266477584839 \n",
      "\n",
      "epoch13, iter320, loss: 2.917147159576416 \n",
      "\n",
      "epoch13, iter330, loss: 3.0367584228515625 \n",
      "\n",
      "epoch13, iter340, loss: 2.9843859672546387 \n",
      "\n",
      "epoch13, iter350, loss: 2.9962663650512695 \n",
      "\n",
      "epoch13, iter360, loss: 2.996760845184326 \n",
      "\n",
      "epoch13, iter370, loss: 3.0202372074127197 \n",
      "\n",
      "epoch13, iter380, loss: 3.0243685245513916 \n",
      "\n",
      "epoch13, iter390, loss: 3.016963481903076 \n",
      "\n",
      "epoch13, iter400, loss: 2.973703145980835 \n",
      "\n",
      "epoch13, iter410, loss: 3.0052149295806885 \n",
      "\n",
      "epoch13, iter420, loss: 3.0396714210510254 \n",
      "\n",
      "epoch13, iter430, loss: 2.988173007965088 \n",
      "\n",
      "epoch13, iter440, loss: 2.950338840484619 \n",
      "\n",
      "epoch13, iter450, loss: 3.032710552215576 \n",
      "\n",
      "epoch13, iter460, loss: 2.959562063217163 \n",
      "\n",
      "epoch13, iter470, loss: 3.0043129920959473 \n",
      "\n",
      "epoch13, iter480, loss: 2.999898910522461 \n",
      "\n",
      "epoch13, iter490, loss: 3.003891944885254 \n",
      "\n",
      "epoch13, iter500, loss: 2.9708850383758545 \n",
      "\n",
      "epoch13, iter510, loss: 2.933530330657959 \n",
      "\n",
      "epoch13, iter520, loss: 2.96384334564209 \n",
      "\n",
      "epoch13, iter530, loss: 2.935594081878662 \n",
      "\n",
      "epoch13, iter540, loss: 3.000279188156128 \n",
      "\n",
      "epoch13, iter550, loss: 2.9198100566864014 \n",
      "\n",
      "epoch13, iter560, loss: 2.941171646118164 \n",
      "\n",
      "epoch13, iter570, loss: 3.0148367881774902 \n",
      "\n",
      "epoch13, iter580, loss: 2.9801714420318604 \n",
      "\n",
      "epoch13, iter590, loss: 3.0271871089935303 \n",
      "\n",
      "Finish epoch 13, time elapsed 1367.4756121635437 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1443.533204\n",
      "Training Check:\tLoss: 2.977378\tAccuracy: 59.855637\tIoU: 0.068000 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 246.168914\n",
      "Validation Results: Loss: 2.983500\tAccuracy: 59.249987\tIoU: 0.065877 \n",
      "\n",
      "epoch14, iter0, loss: 3.0576329231262207 \n",
      "\n",
      "epoch14, iter10, loss: 2.9834036827087402 \n",
      "\n",
      "epoch14, iter20, loss: 2.9850919246673584 \n",
      "\n",
      "epoch14, iter30, loss: 2.998537540435791 \n",
      "\n",
      "epoch14, iter40, loss: 2.9836630821228027 \n",
      "\n",
      "epoch14, iter50, loss: 2.9094090461730957 \n",
      "\n",
      "epoch14, iter60, loss: 3.0187408924102783 \n",
      "\n",
      "epoch14, iter70, loss: 3.042692184448242 \n",
      "\n",
      "epoch14, iter80, loss: 2.9467177391052246 \n",
      "\n",
      "epoch14, iter90, loss: 2.9977166652679443 \n",
      "\n",
      "epoch14, iter100, loss: 3.0708811283111572 \n",
      "\n",
      "epoch14, iter110, loss: 2.8992743492126465 \n",
      "\n",
      "epoch14, iter120, loss: 2.946655035018921 \n",
      "\n",
      "epoch14, iter130, loss: 2.9607818126678467 \n",
      "\n",
      "epoch14, iter140, loss: 2.97835373878479 \n",
      "\n",
      "epoch14, iter150, loss: 2.871204376220703 \n",
      "\n",
      "epoch14, iter160, loss: 2.9594225883483887 \n",
      "\n",
      "epoch14, iter170, loss: 2.948975086212158 \n",
      "\n",
      "epoch14, iter180, loss: 2.9426825046539307 \n",
      "\n",
      "epoch14, iter190, loss: 2.9479284286499023 \n",
      "\n",
      "epoch14, iter200, loss: 2.9660820960998535 \n",
      "\n",
      "epoch14, iter210, loss: 3.0355544090270996 \n",
      "\n",
      "epoch14, iter220, loss: 3.0193734169006348 \n",
      "\n",
      "epoch14, iter230, loss: 2.9837629795074463 \n",
      "\n",
      "epoch14, iter240, loss: 2.9833452701568604 \n",
      "\n",
      "epoch14, iter250, loss: 3.0144777297973633 \n",
      "\n",
      "epoch14, iter260, loss: 2.9976744651794434 \n",
      "\n",
      "epoch14, iter270, loss: 2.883239269256592 \n",
      "\n",
      "epoch14, iter280, loss: 2.9315216541290283 \n",
      "\n",
      "epoch14, iter290, loss: 3.027209520339966 \n",
      "\n",
      "epoch14, iter300, loss: 2.9857401847839355 \n",
      "\n",
      "epoch14, iter310, loss: 3.0097484588623047 \n",
      "\n",
      "epoch14, iter320, loss: 2.983360767364502 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch14, iter330, loss: 2.9778034687042236 \n",
      "\n",
      "epoch14, iter340, loss: 2.9488208293914795 \n",
      "\n",
      "epoch14, iter350, loss: 3.063551425933838 \n",
      "\n",
      "epoch14, iter360, loss: 2.9400269985198975 \n",
      "\n",
      "epoch14, iter370, loss: 2.9524025917053223 \n",
      "\n",
      "epoch14, iter380, loss: 2.9253649711608887 \n",
      "\n",
      "epoch14, iter390, loss: 3.0345895290374756 \n",
      "\n",
      "epoch14, iter400, loss: 2.9916625022888184 \n",
      "\n",
      "epoch14, iter410, loss: 2.9111666679382324 \n",
      "\n",
      "epoch14, iter420, loss: 2.9593634605407715 \n",
      "\n",
      "epoch14, iter430, loss: 2.930403470993042 \n",
      "\n",
      "epoch14, iter440, loss: 2.944467782974243 \n",
      "\n",
      "epoch14, iter450, loss: 2.9555258750915527 \n",
      "\n",
      "epoch14, iter460, loss: 3.0133092403411865 \n",
      "\n",
      "epoch14, iter470, loss: 2.996133804321289 \n",
      "\n",
      "epoch14, iter480, loss: 2.936609983444214 \n",
      "\n",
      "epoch14, iter490, loss: 3.074889659881592 \n",
      "\n",
      "epoch14, iter500, loss: 3.0003089904785156 \n",
      "\n",
      "epoch14, iter510, loss: 3.00288724899292 \n",
      "\n",
      "epoch14, iter520, loss: 2.9109764099121094 \n",
      "\n",
      "epoch14, iter530, loss: 2.987257242202759 \n",
      "\n",
      "epoch14, iter540, loss: 2.9396862983703613 \n",
      "\n",
      "epoch14, iter550, loss: 3.013568639755249 \n",
      "\n",
      "epoch14, iter560, loss: 2.95115327835083 \n",
      "\n",
      "epoch14, iter570, loss: 2.9754574298858643 \n",
      "\n",
      "epoch14, iter580, loss: 3.02351975440979 \n",
      "\n",
      "epoch14, iter590, loss: 2.984863042831421 \n",
      "\n",
      "Finish epoch 14, time elapsed 1367.4699778556824 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1433.348846\n",
      "Training Check:\tLoss: 2.972820\tAccuracy: 60.328620\tIoU: 0.068632 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 246.394529\n",
      "Validation Results: Loss: 2.979525\tAccuracy: 59.663806\tIoU: 0.066821 \n",
      "\n",
      "epoch15, iter0, loss: 2.9875216484069824 \n",
      "\n",
      "epoch15, iter10, loss: 2.9621338844299316 \n",
      "\n",
      "epoch15, iter20, loss: 2.9992423057556152 \n",
      "\n",
      "epoch15, iter30, loss: 2.9422991275787354 \n",
      "\n",
      "epoch15, iter40, loss: 2.926934003829956 \n",
      "\n",
      "epoch15, iter50, loss: 2.9400269985198975 \n",
      "\n",
      "epoch15, iter60, loss: 2.953737497329712 \n",
      "\n",
      "epoch15, iter70, loss: 2.9681007862091064 \n",
      "\n",
      "epoch15, iter80, loss: 2.9517011642456055 \n",
      "\n",
      "epoch15, iter90, loss: 2.9819493293762207 \n",
      "\n",
      "epoch15, iter100, loss: 2.919809103012085 \n",
      "\n",
      "epoch15, iter110, loss: 2.95155668258667 \n",
      "\n",
      "epoch15, iter120, loss: 3.0590038299560547 \n",
      "\n",
      "epoch15, iter130, loss: 2.944017171859741 \n",
      "\n",
      "epoch15, iter140, loss: 3.0013740062713623 \n",
      "\n",
      "epoch15, iter150, loss: 2.9515252113342285 \n",
      "\n",
      "epoch15, iter160, loss: 2.9692537784576416 \n",
      "\n",
      "epoch15, iter170, loss: 2.922053813934326 \n",
      "\n",
      "epoch15, iter180, loss: 2.9375243186950684 \n",
      "\n",
      "epoch15, iter190, loss: 2.9987921714782715 \n",
      "\n",
      "epoch15, iter200, loss: 3.000298023223877 \n",
      "\n",
      "epoch15, iter210, loss: 2.973407745361328 \n",
      "\n",
      "epoch15, iter220, loss: 2.941012144088745 \n",
      "\n",
      "epoch15, iter230, loss: 2.951845645904541 \n",
      "\n",
      "epoch15, iter240, loss: 2.9519126415252686 \n",
      "\n",
      "epoch15, iter250, loss: 2.9391207695007324 \n",
      "\n",
      "epoch15, iter260, loss: 2.945711612701416 \n",
      "\n",
      "epoch15, iter270, loss: 2.973090648651123 \n",
      "\n",
      "epoch15, iter280, loss: 2.998335599899292 \n",
      "\n",
      "epoch15, iter290, loss: 2.9582982063293457 \n",
      "\n",
      "epoch15, iter300, loss: 2.936814785003662 \n",
      "\n",
      "epoch15, iter310, loss: 2.9109878540039062 \n",
      "\n",
      "epoch15, iter320, loss: 2.973148822784424 \n",
      "\n",
      "epoch15, iter330, loss: 2.937488079071045 \n",
      "\n",
      "epoch15, iter340, loss: 2.957646608352661 \n",
      "\n",
      "epoch15, iter350, loss: 2.902318000793457 \n",
      "\n",
      "epoch15, iter360, loss: 2.8846895694732666 \n",
      "\n",
      "epoch15, iter370, loss: 3.048666477203369 \n",
      "\n",
      "epoch15, iter380, loss: 3.0201804637908936 \n",
      "\n",
      "epoch15, iter390, loss: 3.0143935680389404 \n",
      "\n",
      "epoch15, iter400, loss: 2.909755229949951 \n",
      "\n",
      "epoch15, iter410, loss: 2.92223858833313 \n",
      "\n",
      "epoch15, iter420, loss: 2.908281087875366 \n",
      "\n",
      "epoch15, iter430, loss: 3.0007832050323486 \n",
      "\n",
      "epoch15, iter440, loss: 3.0025839805603027 \n",
      "\n",
      "epoch15, iter450, loss: 2.9501242637634277 \n",
      "\n",
      "epoch15, iter460, loss: 3.1340091228485107 \n",
      "\n",
      "epoch15, iter470, loss: 2.9615654945373535 \n",
      "\n",
      "epoch15, iter480, loss: 2.9963412284851074 \n",
      "\n",
      "epoch15, iter490, loss: 2.9281466007232666 \n",
      "\n",
      "epoch15, iter500, loss: 2.9786789417266846 \n",
      "\n",
      "epoch15, iter510, loss: 3.0134432315826416 \n",
      "\n",
      "epoch15, iter520, loss: 2.914529323577881 \n",
      "\n",
      "epoch15, iter530, loss: 3.0011532306671143 \n",
      "\n",
      "epoch15, iter540, loss: 3.0286946296691895 \n",
      "\n",
      "epoch15, iter550, loss: 2.9593842029571533 \n",
      "\n",
      "epoch15, iter560, loss: 2.9810891151428223 \n",
      "\n",
      "epoch15, iter570, loss: 2.931925058364868 \n",
      "\n",
      "epoch15, iter580, loss: 2.904721736907959 \n",
      "\n",
      "epoch15, iter590, loss: 2.9731831550598145 \n",
      "\n",
      "Finish epoch 15, time elapsed 1366.162290096283 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1434.805697\n",
      "Training Check:\tLoss: 2.969166\tAccuracy: 60.665279\tIoU: 0.069796 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 245.882540\n",
      "Validation Results: Loss: 2.976834\tAccuracy: 59.894168\tIoU: 0.067178 \n",
      "\n",
      "epoch16, iter0, loss: 2.9815280437469482 \n",
      "\n",
      "epoch16, iter10, loss: 2.96412992477417 \n",
      "\n",
      "epoch16, iter20, loss: 2.9633994102478027 \n",
      "\n",
      "epoch16, iter30, loss: 3.040574789047241 \n",
      "\n",
      "epoch16, iter40, loss: 2.8978052139282227 \n",
      "\n",
      "epoch16, iter50, loss: 2.96510910987854 \n",
      "\n",
      "epoch16, iter60, loss: 3.0302071571350098 \n",
      "\n",
      "epoch16, iter70, loss: 2.9548373222351074 \n",
      "\n",
      "epoch16, iter80, loss: 2.9968197345733643 \n",
      "\n",
      "epoch16, iter90, loss: 2.992459774017334 \n",
      "\n",
      "epoch16, iter100, loss: 2.9870760440826416 \n",
      "\n",
      "epoch16, iter110, loss: 2.9271442890167236 \n",
      "\n",
      "epoch16, iter120, loss: 2.942124128341675 \n",
      "\n",
      "epoch16, iter130, loss: 3.031588315963745 \n",
      "\n",
      "epoch16, iter140, loss: 3.075181007385254 \n",
      "\n",
      "epoch16, iter150, loss: 2.973806619644165 \n",
      "\n",
      "epoch16, iter160, loss: 2.987356662750244 \n",
      "\n",
      "epoch16, iter170, loss: 2.961733341217041 \n",
      "\n",
      "epoch16, iter180, loss: 2.917306423187256 \n",
      "\n",
      "epoch16, iter190, loss: 2.9351866245269775 \n",
      "\n",
      "epoch16, iter200, loss: 2.9578168392181396 \n",
      "\n",
      "epoch16, iter210, loss: 2.9843742847442627 \n",
      "\n",
      "epoch16, iter220, loss: 2.9168994426727295 \n",
      "\n",
      "epoch16, iter230, loss: 2.9631307125091553 \n",
      "\n",
      "epoch16, iter240, loss: 2.892265796661377 \n",
      "\n",
      "epoch16, iter250, loss: 2.99531626701355 \n",
      "\n",
      "epoch16, iter260, loss: 2.983191967010498 \n",
      "\n",
      "epoch16, iter270, loss: 2.9532294273376465 \n",
      "\n",
      "epoch16, iter280, loss: 3.0400824546813965 \n",
      "\n",
      "epoch16, iter290, loss: 2.963627815246582 \n",
      "\n",
      "epoch16, iter300, loss: 2.923279285430908 \n",
      "\n",
      "epoch16, iter310, loss: 3.037830352783203 \n",
      "\n",
      "epoch16, iter320, loss: 2.93418025970459 \n",
      "\n",
      "epoch16, iter330, loss: 2.869978427886963 \n",
      "\n",
      "epoch16, iter340, loss: 2.91949725151062 \n",
      "\n",
      "epoch16, iter350, loss: 3.0254104137420654 \n",
      "\n",
      "epoch16, iter360, loss: 3.0120930671691895 \n",
      "\n",
      "epoch16, iter370, loss: 2.959357500076294 \n",
      "\n",
      "epoch16, iter380, loss: 2.936201572418213 \n",
      "\n",
      "epoch16, iter390, loss: 3.0169739723205566 \n",
      "\n",
      "epoch16, iter400, loss: 2.9845738410949707 \n",
      "\n",
      "epoch16, iter410, loss: 3.073744773864746 \n",
      "\n",
      "epoch16, iter420, loss: 2.9087631702423096 \n",
      "\n",
      "epoch16, iter430, loss: 3.019120216369629 \n",
      "\n",
      "epoch16, iter440, loss: 3.008416175842285 \n",
      "\n",
      "epoch16, iter450, loss: 2.944376230239868 \n",
      "\n",
      "epoch16, iter460, loss: 3.0349137783050537 \n",
      "\n",
      "epoch16, iter470, loss: 3.0085644721984863 \n",
      "\n",
      "epoch16, iter480, loss: 2.9961540699005127 \n",
      "\n",
      "epoch16, iter490, loss: 2.9993247985839844 \n",
      "\n",
      "epoch16, iter500, loss: 2.96697735786438 \n",
      "\n",
      "epoch16, iter510, loss: 2.9729604721069336 \n",
      "\n",
      "epoch16, iter520, loss: 2.9675087928771973 \n",
      "\n",
      "epoch16, iter530, loss: 3.011044979095459 \n",
      "\n",
      "epoch16, iter540, loss: 3.0866312980651855 \n",
      "\n",
      "epoch16, iter550, loss: 2.9471256732940674 \n",
      "\n",
      "epoch16, iter560, loss: 2.945427656173706 \n",
      "\n",
      "epoch16, iter570, loss: 3.011814594268799 \n",
      "\n",
      "epoch16, iter580, loss: 2.965634822845459 \n",
      "\n",
      "epoch16, iter590, loss: 2.9663662910461426 \n",
      "\n",
      "Finish epoch 16, time elapsed 1369.7957653999329 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 1438.950826\n",
      "Training Check:\tLoss: 2.973643\tAccuracy: 60.234578\tIoU: 0.069093 \n",
      "\n",
      "Starting Evaluation\n",
      "Finished evaluation. Time elapsed 245.542048\n",
      "Validation Results: Loss: 2.980102\tAccuracy: 59.598779\tIoU: 0.067020 \n",
      "\n",
      "epoch17, iter0, loss: 2.9353997707366943 \n",
      "\n",
      "epoch17, iter10, loss: 3.0287365913391113 \n",
      "\n",
      "epoch17, iter20, loss: 2.9760043621063232 \n",
      "\n",
      "epoch17, iter30, loss: 2.9306607246398926 \n",
      "\n",
      "epoch17, iter40, loss: 2.9555976390838623 \n",
      "\n",
      "epoch17, iter50, loss: 2.9797825813293457 \n",
      "\n",
      "epoch17, iter60, loss: 2.9386837482452393 \n",
      "\n",
      "epoch17, iter70, loss: 2.958775043487549 \n",
      "\n",
      "epoch17, iter80, loss: 2.9899678230285645 \n",
      "\n",
      "epoch17, iter90, loss: 2.979846954345703 \n",
      "\n",
      "epoch17, iter100, loss: 2.9779772758483887 \n",
      "\n",
      "epoch17, iter110, loss: 2.962411403656006 \n",
      "\n",
      "epoch17, iter120, loss: 2.9810993671417236 \n",
      "\n",
      "epoch17, iter130, loss: 2.9380736351013184 \n",
      "\n",
      "epoch17, iter140, loss: 2.9100329875946045 \n",
      "\n",
      "epoch17, iter150, loss: 2.926917791366577 \n",
      "\n",
      "epoch17, iter160, loss: 2.936957359313965 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch17, iter170, loss: 2.9125142097473145 \n",
      "\n",
      "epoch17, iter180, loss: 2.960763454437256 \n",
      "\n",
      "epoch17, iter190, loss: 2.907597780227661 \n",
      "\n",
      "epoch17, iter200, loss: 2.986754894256592 \n",
      "\n",
      "epoch17, iter210, loss: 2.93454647064209 \n",
      "\n",
      "epoch17, iter220, loss: 2.9923348426818848 \n",
      "\n",
      "epoch17, iter230, loss: 2.969949245452881 \n",
      "\n",
      "epoch17, iter240, loss: 2.997894048690796 \n",
      "\n",
      "epoch17, iter250, loss: 2.991730213165283 \n",
      "\n",
      "epoch17, iter260, loss: 3.026193141937256 \n",
      "\n",
      "epoch17, iter270, loss: 2.9891841411590576 \n",
      "\n",
      "epoch17, iter280, loss: 3.006556272506714 \n",
      "\n",
      "epoch17, iter290, loss: 2.99703049659729 \n",
      "\n",
      "epoch17, iter300, loss: 2.950599431991577 \n",
      "\n",
      "epoch17, iter310, loss: 2.933583974838257 \n",
      "\n",
      "epoch17, iter320, loss: 2.9980721473693848 \n",
      "\n",
      "epoch17, iter330, loss: 3.0574707984924316 \n",
      "\n",
      "epoch17, iter340, loss: 2.9793028831481934 \n",
      "\n",
      "epoch17, iter350, loss: 3.0018606185913086 \n",
      "\n",
      "epoch17, iter360, loss: 2.9870638847351074 \n",
      "\n",
      "epoch17, iter370, loss: 2.999814748764038 \n",
      "\n",
      "epoch17, iter430, loss: 2.933197259902954 \n",
      "\n",
      "epoch17, iter440, loss: 2.994652271270752 \n",
      "\n",
      "epoch17, iter450, loss: 2.950007200241089 \n",
      "\n",
      "epoch17, iter460, loss: 3.0457262992858887 \n",
      "\n",
      "epoch17, iter470, loss: 3.0401768684387207 \n",
      "\n",
      "epoch17, iter480, loss: 3.031499147415161 \n",
      "\n",
      "epoch17, iter490, loss: 2.9382081031799316 \n",
      "\n",
      "epoch17, iter500, loss: 3.008894443511963 \n",
      "\n",
      "epoch17, iter510, loss: 2.9562010765075684 \n",
      "\n",
      "epoch17, iter520, loss: 2.987316370010376 \n",
      "\n",
      "epoch17, iter530, loss: 3.021609306335449 \n",
      "\n",
      "epoch17, iter540, loss: 2.9987525939941406 \n",
      "\n",
      "epoch17, iter550, loss: 2.980417251586914 \n",
      "\n",
      "epoch17, iter560, loss: 2.974682569503784 \n",
      "\n",
      "epoch17, iter570, loss: 2.973339557647705 \n",
      "\n",
      "epoch17, iter580, loss: 3.006377935409546 \n",
      "\n",
      "epoch17, iter590, loss: 2.930826425552368 \n",
      "\n",
      "Finish epoch 17, time elapsed 1371.7265667915344 \n",
      "\n",
      "Starting Evaluation\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu:\n",
    "    fcn_model = fcn_model.to(device)\n",
    "    \n",
    "best_loss = float('inf')\n",
    "prev_loss = float('inf')\n",
    "loss_inc_cnt = 0\n",
    "stop_early = False\n",
    "\n",
    "def train():\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    print(\"Starting Training\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        ts = time.time()\n",
    "        for iter, (X, tar, Y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs = X.to(device)\n",
    "                labels_cat = Y.to(device)\n",
    "            else:\n",
    "                inputs, labels_cat, labels_enc = X, Y, tar\n",
    "\n",
    "            outputs = softmax(fcn_model(inputs))\n",
    "            loss = criterion(outputs, labels_cat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print_info(\"epoch{}, iter{}, loss: {} \\n\".format(epoch, iter, loss.item()))\n",
    "                \n",
    "        \n",
    "        print_info(\"Finish epoch {}, time elapsed {} \\n\".format(epoch, time.time() - ts))\n",
    "    \n",
    "        loss, acc, IoU = evaluate(train_loader)\n",
    "\n",
    "        print_info(\"Training Check:\\tLoss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "        \n",
    "        val(epoch)\n",
    "        if stop_early: return\n",
    "  \n",
    "#         fcn_model.train()\n",
    "\n",
    "def evaluate(data_loader, validation=False, verbose=False):\n",
    "\n",
    "    global best_loss\n",
    "    global prev_loss\n",
    "    global loss_inc_cnt\n",
    "    global stop_early\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accs = []\n",
    "        ious = []\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        ts = time.time()\n",
    "        print(\"Starting Evaluation\")\n",
    "        \n",
    "        for iter, (X, tar, Y) in enumerate(data_loader):\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs = X.to(device)\n",
    "                labels_cat = Y.to(device)\n",
    "            else:\n",
    "                inputs, labels_cat, labels_enc = X, Y, tar\n",
    "\n",
    "            outputs = fcn_model(inputs)\n",
    "            outputs = softmax(outputs)\n",
    "\n",
    "            output_labels = outputs.argmax(dim=1)\n",
    "\n",
    "            losses.append(criterion(outputs, labels_cat).item())\n",
    "\n",
    "            accs.append(pixel_acc(output_labels, labels_cat))\n",
    "\n",
    "            ious.append(np.nanmean(iou(output_labels, labels_cat)))\n",
    "\n",
    "            loss = np.mean(losses)\n",
    "            acc = np.mean(accs)\n",
    "            IoU = np.mean(ious)\n",
    "\n",
    "            if verbose: print(\"Batch %d:\\tLoss: %f\\tAccuracy: %f\\tIoU: %f\" % (iter, loss, acc * 100, IoU))\n",
    "            \n",
    "\n",
    "        print(\"Finished evaluation. Time elapsed %f\" % (time.time() - ts))\n",
    "\n",
    "        # This probably should not be a straight average, but just doing this for now\n",
    "        loss = np.mean(losses)\n",
    "        acc = np.mean(accs)\n",
    "        IoU = np.mean(ious)\n",
    "        \n",
    "        if validation:\n",
    "            if best_loss > loss:\n",
    "                best_loss = loss\n",
    "                print_info(\"Best Loss: \" + str(best_loss) + \"\\n\")\n",
    "                torch.save(fcn_model.state_dict(), best_model_fn)\n",
    "            loss_inc_cnt = loss_inc_cnt + 1 if prev_loss < loss else 0\n",
    "            if loss_inc_cnt > 3: stop_early = True\n",
    "        \n",
    "        return loss, acc, IoU\n",
    "\n",
    "def val(epoch):\n",
    "    # fcn_model.eval()\n",
    "    # Complete this function - Calculate loss, accuracy and IoU for every epoch\n",
    "    # Make sure to include a softmax after the output from your model\n",
    "    loss, acc, IoU = evaluate(val_loader, validation=True)\n",
    "    print_info(\"Validation Results: Loss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "    if stop_early: print_info(\"Epoch %d:\\tStopping Early\" % (epoch))\n",
    "    \n",
    "def test():\n",
    "    print(' ')\n",
    "    # Complete this function - Calculate accuracy and IoU \n",
    "    # Make sure to include a softmax after the output from your model\n",
    "    loss, acc, IoU = evaluate(test_loader)\n",
    "    print_info(\"Test Results:\\tLoss: %f\\tAccuracy: %f\\tIoU: %f \\n\" % (loss, acc * 100, IoU))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "#     val(0)  # show the accuracy before training\n",
    "#     print_info(\"---------Above is accuracy before training.---------\\n\")\n",
    "    train()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
